---
title: "[논문리뷰] DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
date: 2025-07-23
last_modified_at: 2025-07-23
categories:
  - 논문리뷰
tags:
  - Reinforcement Learning
  - Reasoning
  - LLM
excerpt: "DAPO"
use_math: True
classes: wide
---
> arXiv 2025. [[Paper](https://arxiv.org/abs/2503.14476)] 
> 20 May 2025

# Preliminary: GRPO

$$
\mathcal{J}_{GRPO}(\theta)=\mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim\pi_{\theta_{old}}(\cdot\vert q)}\Bigg[ \frac{1}{G}\sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert} \min \left( r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(r_{i,t}(\theta), 1-\epsilon,1+\epsilon)\hat{A}_{i,t}-\beta D_{KL}(\pi_\theta\vert\vert\pi_{ref}) \right) \Bigg]
$$


where

$$
r_{i,t}(\theta)=\frac{\pi_\theta(o_{i,t}\vert q,o_{i,<t})}{\pi_{old}(o_{i,t}\vert q,o_{i,<t})},\qquad \hat{A}_{i,t}=\frac{R_i-\text_{mean}({R_i}_{i=1}^G)}{\text{std}({R_i}_{i=1}^G)} 
$$

# Decoupule Clip and Dynamic Sampling Policy Optimization (DAPO)
Sampling: $(q,a)\sim \{o_i\}_{i=1}^G$

$$
\begin{align*}
\mathcal{J}_{DAPO}(\theta)=&\mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim\pi_{\theta_{old}}(\cdot\vert q)}\Bigg[ \frac{1}{\sum_{i=1}^G\vert o_i\vert}\sum_{i=1}^G\sum_{t=1}^{\vert o_i\vert} \min \left( r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(r_{i,t}(\theta), 1-\epsilon_{low},1+\epsilon_{high})\hat{A}_{i,t} \right) \Bigg] \\
\text{s.t. } 0 < & \vert\{o_i\vert \text{is_equivalent}(a,o_i)\}\vert < \vert G\vert 
\end{align*}
$$


where

$$
r_{i,t}(\theta)=\frac{\pi_\theta(o_{i,t}\vert q,o_{i,<t})}{\pi_{old}(o_{i,t}\vert q,o_{i,<t})},\qquad \hat{A}_{i,t}=\frac{R_i-\text_{mean}({R_i}_{i=1}^G)}{\text{std}({R_i}_{i=1}^G)} 
$$

위의 식에서도 알 수 있듯이 기존 GRPO 와는 다른 점이 존재한다. 하나씩 알아본다.

![](/assets/img/DAPO/dapo.webp)
 
## Raise the Ceiling: Clip-Higher
기존 PPO/GRPO 등을 사용했을 시에는 생성된 response가 training을 하면 할수록 서로 동일해지는 현상을 보였다. (entropy 감소) Exploration 이 제한됨을 알 수 있다. 본 논문에서는 그 이유가 clipping 의 upper bound 때문으로 생각하고 있다. clipping 은 학습을 안정화하기 위해 도입했으나 너무 낮은 upper bound는 exploration 을 제한한다. 더 정확히 말하면 확률이 높은 토큰은 더 확률이 높게 만들고 확률이 낮은 토큰은 확률을 더 낮게 만든다. 

e.g. $\epsilon = 0.2, \pi_{\theta_{old}}(o_i\vert q)=0.01 \text{and } 0.9$ => $\pi_{\theta_{old}}\cdot(1+\epsilon)=0.012 \text{and } 1.08$, respectively.

위의 예제에서 알 수 있듯이 확률이 낮은 토큰은 같은 importance ratio상에서 새로운 policy의 확률 값이 증가하는 정도가 확률이 높은 토큰에 비해 작다. 그래서 원래 확률이 높았던 토큰 쪽으로 더 생성하도록 계속 학습되는 현상이 발생하고 exploration이 제한된다.Training을 하면 할수록  clipped probability의 평균 값은 감소한다. 즉 모델이 계속 똑같은 답만 생성하는 쪽으로 학습된다. DAPO 는 이러한 문제를 해결하기 위해 $\epsilon$ 값을 decouple 시켜 lower bound 와 upper bound 의 값을 달리하고 upper bound $\epsilon_{high}$ 값을 증가시켜 다양한 토큰을 생성하게 한다.

## Dynamic Sampling (zero mean)
만약 특정 prompt에 대한 output들이 모두 동일하면 다 같은 reward를 받게 될거고 advantage 가 0이 되어 버린다. 그러면 해당 group에 대한 advantage가 0이 되니 gradient 도 0이 되어 해당 batch의 gradient magnitude가 감소하고 nosie가 증가한다. Training을 하면 할 수록 정확도가 1인 prompt의 수가 증가하는데 이는 training에 도움이 되는 prompt의 수가 감소하는 것을 의마한다. 이렇게 되면 gradient의 variance가 커져서 이후 gradient signal의 안정성이 떨어진다. 

이를 해결하기 위해, DAPO는 실제 batch size보다 over sampling을 한 후, group 내의 정확도가 0이거나 1인 prompt 를 필터링하는 식으로 학습한다. 그래서 DAPO objective 에는 부등식 조건이 추가되었다. 정확도가 0과 1 사이인 sample이 batch size만큼 생성될 때까지 반복적으로 sampling한다. 이렇게 하면 sampling overhead가 발생하지만 학습이 안정적이기 때문에 같은 성능에 더 빨리 도달할 수 있다.

## Rebalancing Act: Token-Level Policy Gradient loss
GRPO는 loss 를 계산할 때 각 sample (prompt-answer) 내에서 token 별로 loss 를 계산하고 group 내에서 평균을 내는 방식을 이용한다. 그렇기 때문에 모든 sample에 동일한 가중치를 부여한다: $\frac{1}{G}$ 동일한 가중치를 부여한다는 것은 사실 sample-level에서 계산한 loss 비율만큼 total loss에 그대로 부여된단 이야기이다. 

예를 들어 loss가 [2,0] 인 sample과 [0.5]x10 인 sample을 비교해봤을 때 loss가 1/0.5로 계산이 될 것이다. 그럼 전자는 둘 중 하나를 제대로 생성했고 후자는 10 토큰 중 하나도 제대로 생성하지 못했는데 전자가 더 큰 loss를 total loss에 기여하게 될 것이다. 

위의 예시와 같이 쓸데없이 길이만 길고 아무 의미없는 패턴에 대한 penalty가 낮아 길이가 의미 없이 길어지고 반복적인 패턴을 생성하는 경향을 보이게 된다. 또, 질이 좋고 길이가 긴 sample의 경우 reasoning 과정에서 중요한 token들에 대한 학습 signal이 sample loss를 계산할 때 희석되어 유익한 reasoning 패턴을 학습하지 못하게 된다.

이를 해결하기 위해 DAPO는 group 내에서 token level로 loss를 계산하고 sample내에서 평균을 내는 것이 아니라 그냥 token-level에서 평균을 계산한다. 고로 길이가 긴 response는 gradient update에 상대적으로 더 큰 영향을 끼치게 된다. 또, 각 token에 대해서 reward를 많이 받는 token은 더 생성되게 될 것이고 반대의 경우 더 생성되지 않게 될 것이다.

### Overlong Reward Shaping
LLM 은 기본적으로 max ouput length를 두고 있다. Max output length를 넘어가는 token들 (truncated sample) 에 대해서 penalty를 준다거나 하면 좋은 reasoning token임에도 불구하고 reward를 낮게 받아서 일종의 noise가 될 수도 있고 학습에 방해가 될 수 있다.

이러한 문제를 해결하기 위해 max length를 넘어가는 token들에 대해서는 loss를 계산하지 않으니 훨씬 더 안정적인 학습 + 성능 개선을 이뤄낼 수 있었다. (Overlong Filtering)

또 다른 방법은, truncated sample에 대해서 penalty를 주는 대신에 max output length를 넘어선 정도가 크면 클수록 더 큰 penalty를 받게 설계했다.

$$
R_{length}(y)=\begin{cases}
  0, & \vert y \vert \leq L_{max}-L_{cache} \\
  \frac{(L_{max}-L_{cache})-\vert y \vert }{L_{cache}}, & L_{max}-L_{cache} < \vert y \vert \leq L_{max}\\
  -1, & L_{max} < \vert y\vert \\
\end{cases}
$$

이를 기존 rule-based correctness reward에 더해서 계산한다.