---
title: "[논문리뷰] Accelerating RLHF Training with Reward Variance Increase"
date: 2025-07-25
last_modified_at: 2025-07-25
categories:
  - 논문리뷰
tags:
  - Reinforcement Learning
  - Reasoning
  - LLM
excerpt: "GRPOVI"
use_math: True
classes: wide
---
> arXiv 2025. [[Paper](https://arxiv.org/abs/2505.23247)] 
> 18 Jun 2025

# Preliminaries: RHLF
Prompt/Output: $\mathbf{x}\in\mathcal{X}, \mathbf{y}\in\mathcal{Y}\subseteq\mathcal{V}$. Policy model: $\pi_\theta$ 라고 하자. SFT 를 이용해 base model $\pi_{\theta_{ref}}$을 학습한다. RHLF는 human preference를 담고 있는 ground truth reward가 있다고 가정하고 policy model이 생성하는 output이 이 ground truth reward, 즉 human reference를 최대로 하게 학습하는 것을 목표로 한다. 하지만 ground truth reward는 알 수 없기 때문에 reward model을 먼저 학습한 뒤, reward model과 policy gradient method를 이용해 policy model을 학습한다. 

$$
\phi_{RLHF}(\theta):=\mathbb{E}_{\mathbf{x}\sim \mathcal{S}}\left[\mathbb{E}_{\mathbf{y}\sim\pi_\theta(\cdot\vert\mathbf{x})}[r_{RM}(\mathbf{x,y})]-\lambda\cdot \text{KL}(\pi_\theta(\cdot\vert\mathbf{x})\Vert\pi_\theta(\cdot\vert\mathbf{x}))\right]
$$

RLHF는 proxy reward의 expectation을 maximize함을 통해 expected ground truth reward 또한 증가하는 것을 목표로 하고 있다. Reward model의 경우 두 가지 metric을 이용해 평가된다.

- Accuracy: $acc_{\mathbf{x},\mathcal{D}}(r\_{RM}):=\mathbb{E}\_{\{\mathbf{y,y'}\}\sim\mathcal{D}}\Bigg[\mathbb{1}\Big[sign(r\_{RM}(\mathbf{x,y})-r\_{RM}(\mathbf{x,y'}))=sign(r\_G(\mathbf{x,y})-r\_G(\mathbf{x,y'}))\Big]\Bigg]$
- Variance: 
![](/assets/img/RV/var.webp)

Reward Variance가 높을 수록 RHLF training 속도가 증가한다. GRPOVI는 Reward adjustment model 이용. Expected reward, reward preference (순서) 는 유지하면서 reward variance를 최대화해서 RHLF (GRPO) training 최적화
