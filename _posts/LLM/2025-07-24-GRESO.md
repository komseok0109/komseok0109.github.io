---
title: "[논문리뷰]Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts"
date: 2025-07-24
last_modified_at: 2025-07-24
categories:
  - 논문리뷰
tags:
  - Reinforcement Learning
  - Reasoning
  - LLM
excerpt: "GRESO"
use_math: True
classes: wide
---
> arXiv 2025. [[Paper](https://arxiv.org/abs/2506.02177)] [[Code](https://github.com/Infini-AI-Lab/GRESO)]
> 2 Jun 2025

특정 prompt에서 생성되는 response들의 reward가 모두 동일한 경우 advantage가 0이라서 학습에 전혀 도움이 되지 않는다. 이를 해결하기 위해 [DAPO](https://arxiv.org/abs/2503.14476)는 dynamic sampling을 도입했으나 rollout 이후에 검증하는 것이기 때문에 overhead가 크다. 다음 세 가지 조건을 만족해야한다.

- Online data selection: training 과정에서 선별할 수 있어야 한다. Offline에서 fine-tuning된 모델을 이용해 dataset pruning을 하는 것은 그만큼 cost가 발생하기 때문이다.
- Model-based data value estimation: 모델/training stage에 맞게 dynamic하게 data의 value를 측정하는 방법이 필요하다. (어떤 data가 informative한지, 학습에 도움이 되는지)
- Low Computational Overhead: overhead가 적어야 한다.

# Observations related to zero-variance prompts
특정 prompt에 대해서 생성한 response의 reward가 variance, advantage 모두 0이 되기 때문에 해당 prompt는 더 이상 학습에 도움이 되지 않는다. 이를 zero-variance prompt라 하고 반대의 경우를 effective prompt 라고 부르기로 한다.

![](/assets/img/GRESO/ob1.webp)

위 그래프에서 확인할 수 있듯이 effective prompt의 비율이 학습을 할수록 감소한다. 이는 가면 갈수록 학습에 의미없는 rollout이 생성되고 있다는 이야기이고 효율성 측면에서 좋지 못하다. DAPO는 dynamic sampling을 도입해서 문제를 해결했고 성능/효율성 모두 개선했다. 이는 zero-variance prompt를 제거하는 것이 성능/효율성 모두 개선하는데 효과가 있음을 증명한다. 하지만, dynamis sampling은 기존 GRPO 보다는 효율적이지만 oversampling cost가 발생한다. Rollout을 생성하기 전에 prompt만 보고 이를 확인할 수 있다면 더 효율적인 학습이 가능할 것이다.

또, training data는 epoch 간에 temporal correlation이 존재하는 것은 이미 잘 알려져 있다. 본 논문에서는 zero-variance prompt도 비슷한 특성을 지닐 것으로 예상해서 이를 확인해보고 있다. 다음 두 가지 값을 조사해본다.

- $P(Previous\vert Current)$: 현재 epoch에서 zero variance prompt가 이전 epoch에서도 zero variance prompt일 확률 
- $P(Current\vert Previous)$: 이전 epoch에서 zero variance prompt가 현재 epoch에서도 zero variance prompt일 확률 

![](/assets/img/GRESO/ob2.webp)

- 주황색 그래프에서 확인할 수 있듯이  현재 epoch에서 zero-variance인 경우 이전 epoch에서도 그러할 확률이 매우 높다.
- 초록색 그래프에서 우리는 이전에 zero-variance prompt였어도 20% 정도는 effective prompt임을 확인할 수 있다. 이는 rollout 과정에서 exploration을 통해 effective prompt로 변환할 수 있는 가능성을 제시하고 있다.
 
# GRPO with Efficient Selective Rollout
![](/assets/img/GRESO/greso.webp)

위에서 zero-variance prompt는 다음 epoch에서도 zero-variance prompt일 확률이 높은걸 관찰했다. 그래서 epoch마다 rollout하기 전에 zero-variance prompt를 필터링한다. 각 prompt $x_i$ 에 대하여 training dynamics trace 를 다음과 같이 정의한다.

$$
T_i=(e_{i,1}, R_{i,1}),\dots,(e_{i,n}, R_{i,n}) \text{ where } e_{i,j}: \text{epoch, } R_{i,j}=\{r_{i,j}^{(k)}\}_{k=1}^G: \text{reward}
$$

Rollout 을 생성하기 전에 위에 training dynamics 를 이용해 zero-variance prompt인지 예측하는 것이 목표이다. 본 논문에서는 probabilistic filtering 을 이용한다. Training dynamics 를 바탕으로 filtering probability 를 계산한다. 이전에 zero-varaince prompt 였다고 해도 현재 epoch 에서는 effective prompt 일 수도 있기 때문에 zero-varaince prompt도 가끔 sampling되게 해서 exploration와 exploitation 의 balance 를 맞출 수 있게 probability 값이 계산되어야 한다. Filtering Probability:

$$
p_f(x_i)=1-p_e^{z_i} 
$$

$$
z_i=\max\left\{k\in[0,n]\Bigg| \prod_{j=n-k+1}^n \mathbb{I}_{i,j}=1\right\}
$$

$$
\mathbb{I}_{i,j}=\begin{cases}
  1, & \text{if all rewards in $R_{i,j}$ are identical,} \\
  0, & \text{otherwise,}
\end{cases}
$$

$p_e$ 는 exploration 을 얼마나 할지 조절하는 probability 값이고 $z_i$ 는 식에서 알 수 있듯이 zero-variance rollout 을 얼마나 연속으로 생성했는지를 나타내고 있다. 즉 최근 zero-variance rollout을 연속적으로 생성한 횟수가 많을수록 filtering probability 의 값은 커질 것이다.

$p_e$ 의 값을 결정하는 것이 GRESO 알고리즘에서 어려운 점이다. 적절한 값이 dataset마다, model마다, model/dataset이 동일해도 epoch마다 다 다르기 때문이다. 또 같은 zero-variance prompt여도 prompt 자체적인 난이도에 따라 다른 값을 가지는 것이 합리적이지만 값을 이렇게 하나하나 조절하는 것은 현실적으로 어렵다. 대신에 target zero-variance percentage 를 입력으로 받아 target 보다 낮으면 exploration rate 를 올리고 높으면 exploration rate 를 낮춘다. (step size $=\nabla_p$) 이때, $p_easy$ 와 $p_hard$ 를 구분해서 가지고 있다가 filtering probability 를 계산하기 전에 prompt의 난이도를 결정한 후 적절한 값을 선택한다. 본 논문에서 실험을 할 때에는 target zero-variance ratio 를 25%로 설정하고 easy는 8.3, hard는 16.7을 이용했다. 이렇게 하는 이유는 hard prompt 를 더 exploration하는 것이 더 합리적이라고 생각했기 때문이다.

마지막으로, DAPO에서는 zero-varaince prompt 인 경우 새로 sampling 해서 정해진 batch size 만큼을 맞추고 policy update를 진행했다. 하지만 batch size에 비해 턱없이 부족하면 모르겠지만 그렇지 않은 경우에는 computation 낭비이기 때문에 GRESO 는 batch size를 dynamic하게 가져간다.

$$
\text{Rollout batch size: }B_r=\min (B_r^{default}, \frac{\beta B_\nabla}{(1-\alpha)})
$$

$B_\nabla$ 는 training batch 를 다 채우기 위한 example의 개수이다. $\alpha$ 는 zero-variance example ratio이다. 

![](/assets/img/GRESO/algo.webp)