---
title: "[논문리뷰] Absolute Zero: Reinforced Self-play Reasoning with Zero Data"
date: 2025-07-17
last_modified_at: 2025-07-17
categories:
  - 논문리뷰
tags:
  - Reinforcement Learning
  - Reasoning
  - LLM
excerpt: "AZR"
use_math: True
classes: wide
---
> arXiv 2025. [[Paper](https://arxiv.org/abs/2505.03335)] [[Code](https://github.com/LeapLabTHU/Absolute-Zero-Reasoner)]
> 13 Jun 2025

# Background
SFT: Given query-CoT-answer dataset $\mathcal{D}=\{(x,c^\star,y^\star)\}$, minimize the following obejctive:

$$\mathcal{L}_{SFT}(\theta)=-\mathbb{E}_{(x,c^\star,y^\star)\sim\mathcal{D}}\log\pi_\theta(c^\star,y^\star\vert x) $$

SFT 는 human expert labeling 혹은 superior AI 모델에서 distillation 이 필요하지만, frontier model은 superior model이 없고 human labeling은 cost가 매우 크다.

RLVR: Given query-answer dataset $\mathcal{D}=\{(x,y^\star)\}$ and verifiable reward function $r$, policy is optimized to maximize expected reward:

$$\mathcal{J}_{RLVR}(\theta)=\mathbb{E}_{(x,y^\star)\sim\mathcal{D},y\sim\pi_\theta(\cdot\vert x)}[r(y,y^\star)] $$

RLVR 은 GT reasoning 은 필요하지 않지만 task distribution $(x,y^\star)$ 에 대한 human-curated dataset이 여전히 필요하다.

# Absolute Zero
Absoulte zero 는 기존의 SFT/RLVR 과 달리, 학습하는 동안 모델이 task를 직접 제안하고, 해결하며 그 과정에서 스스로 학습을 하는 zero data paradigm이다.

![](/assets/img/AZR/loop.webp)

학습 중 하나의 모델이 prposer $\pi_\theta^{propose}$, solver $\pi_\theta^{solve}$ 두 가지 역할을 한다. Proposer가 먼저 task $\tau\sim\pi_\theta^{propose}(\cdot\vert z)$ 를 sampling 한다. Task는 environment $e$ 를 거쳐 validation 이후 reasoning task (task query, golden asnwer) 로 변환된다: $(x,y^\star)\sim f_e(\cdot\vert\tau)$. Solver는 정답을 생성한다. $y\sim\pi_\theta^{solve}(\cdot\vert x)$. policy는 다음 두 가지 보상을 받는다
- Learnability Reward: $r_e^{propose}(\tau, \pi_\theta)$. 제안된 task로 학습한 후 policy가 얼마나 개선되었는지 측정한다.
- Soltuion Reward: $r_e^{solve}(y,y^\star)$. 정답을 맞추었는지 판단한다.

Absolute Zero Objective:

$$\mathcal{J}_\theta:=\max_\theta\mathbb{E}_{z\sim p(z)}\left[\mathbb{E}_{(x,y^\star)\sim f_e(\cdot\vert\tau),\tau\sim\pi_\theta^{propose}(\cdot\vert z)}\left[r_e^{propose}(\tau,\pi_\theta)+\lambda\mathbb{E}_{y\sim\pi_\theta^{solve}(\cdot\vert x)}[r_e^{solve}(y,y^\star)]\right]\right]$$

$\lambda$는 모델이 새로운 task를 더 찾을지 혹은 reasoning 능력을 개선할지에 대한 trade-off를 조절하는 값이다. Human expert data scaling 대신에 policy와 environment를 이용해 task와 grounded feedback을 생성하기 때문에 **ZERO DATA** 이다. 

$z$ 의 경우, task $\tau$ 를 생성할 때 conditional variable 처럼 사용되는데 실제 학습 과정에서는 생성된 (task, answer) pair를 메모리에 저장해놓고 sampling 하는 방식을 사용할 수 있다.

# AZR (Absoulte Zero Reasoner)
![](/assets/img/AZR/overview.webp)

각 online rollout iteration 마다, AZR 은 이전에 생성한 $K$개의 sample들과 task type에 conditioning 해서 새로운 task를 생성한다. 이 task들은 이전의 sample들과는 달라서 다양성과 coverage를 높일 수 있다. Task proposal 들은 filtering을 거친 후에 valid reasoning task로 변환된다. 이후 proposed task에 대한 response를 생성한 후, feedback을 받는다.

## Reward Design
Proposer reward의 경우 너무 풀기 쉽지도 않지만 그렇다고 풀 수 없는 정도는 아닌 task를 생성할 때 reward를 받을 수 있도록 설계한다. n개의 Monte Carlo rollout을 생성하고 성공율을 계산한다: $\bar{r}\_{solve}=\frac{1}{N}\sum_{i=1}^N r_{solve}^{(i)}$ 1-성공률로 reward를 계산한다. 

$$r_{propose}=\begin{cases}
  0, &\text{if }\bar{r}_{solve}=0\text{ or }\bar{r}\_{solve}=1 \\
  1-\bar{r}_{solve}, &\text{otherwise},
\end{cases}$$

Solver reward의 경우 output을 맞췄는지 여부를 이용해 binary reward를 계산한다. Solver가 생성한 정답과 ground truth answer이 같은지 여부를 Python을 이용해 확인한다.

$$r_{solve}=\mathbb{I}_{(y=y^\star)}$$

DeepSeek-R1과 유사하게 format-aware penalty를 추가해서 reward를 계산한다.

$$R(y_\pi)\begin{cases}
  r_{\text{role}} & \text{if the response is passable, role} \in \{\text{propose,solve}\} \\
  -0.5 & \text{if the response is wrong but well-formatted}\\
  -1 & \text{if the answer has formatting errors} \\
\end{cases}$$

## Learning Different Modes of Reasoning
AZR 은 coding task를 이용해 학습되었다. Coding task를 사용한 이유는 code-based 학습이 reasoning 성능을 개선할 수 있고, coding task 자체가 turing-complete하기 때문이다.

AZR reasoning task는 program, input, output triplet $(p,i,o),o=p(i)$ 로 정의된다. Task는 triplet의 두 가지 part가 주어졌을 때 나머지 한 part를 infer하도록 정의된다.
- Deduction: Program과 input이 주어졌을 때 output을 추론한다. Proposer는 $\alpha=deduction$ 과 buffer $\mathcal{D}_{deduction}$에서 sampling 한 $K$ 개의 reference example이 주어지면 새로운 program, input pair를 생성한다.  Python을 이용해 주어진 pair에 대한 ground output을 생성한다. 에러 없이 output이 계산되었다면 buffer에 추가된다. Solver는 자신이 생성한 output과 ground output이 맞는지 검사한다.
- Abduction: Program과 output이 주어졌을 때 input을 추론한다. Proposer는 deducion과 동일하게 $\alpha=abduction$ 과 buffer $\mathcal{D}_{abduction}$에서 sampling 한 $K$ 개의 reference example이 주어지면 새로운 program, input pair를 생성한다. Python을 이용해 주어진 pair에 대한 ground output을 생성한다. 에러 없이 output이 계산되었다면 buffer에 추가된다. 대신 sovler는 problem, output pair를 입력으로 받는다.solver는 input을 추론하고 python을 이용해 input을 program에 넣어 output이 제대로 생성되는지 확인한다.
- Induction: input-output pair에서 program을 추론하는 문제이다. Proposer는 $\mathcal{D}\_{abduction}\cup\mathcal{D}\_{deduction}$ 에서 valid program p를 sampling하고 N개의 새로운 input과 message $m$을 생성한다. 그리고 code executor를 활용해 각 input마다 output을 생성한다. 결론적으로는 $(p,\{(i^n,o^n)\}),m$이 생성된다. 생성된 representation은 induction buffer $\mathcal{D}_{induction}$에 저장된다. message를 이용하는 이유는 주어진 input output을 만들어내는 program은 셀 수 없이 많기 때문에 일종의 condition으로 사용된다. Solver는 $m$과 input-output set의 절반을 입력으로 받고 program을 생성한다. 생성된 program은 나머지 input-output pair를 만족하는 program이어야 한다.

## Algorithm
![](/assets/img/AZR/algo.webp)

먼저, 각 buffer를 seed set $\mathcal{D}_{seed}$를 이용해 초기화한다. Seed의 개수는 $B (\text{batch size}) \times 4$  이다. 각 buffer를 생성하는 과정은 위에서 각 task의 설명에서 proposer가 하는 과정과 동일하다. 대신에 이때는 model update는 없다. 

위에서 설명한 대로, buffer는 deduction/abduction 에서는 promptingtl in-context example로 사용된다. induction에서는 N개의 input, output과 message를 생성하는데 이용된다. 

위에서 각 task는 valid한 pair만 solver에게 입력으로 제공된다고 했다. valid한지 여부는 아래 3가지를 검사해서 결정한다.
- Program Integrity: Python을 이용해 $p(i)$를 돌려본다. 에러가 없고 return이 발생했다면 최소 syntax는 valid하다고 할 수 있다.
- Program Safety: rule-based 방식으로 안전하지 않은 라이브러리를 사용하고 있는지 확인한다.
- Check for Determinisim: output이 deterministic한지 확인한다. Determinisitic하지 않으면 매번 answer가 달라지니 verifiable function을 이용해 reward를 계산하는 것이 논리적으로 어색하기 때문이다. Deterministic의 정의는 무한번 돌려봤을 때 항상 결과가 같아야 하지만 무한 번 돌려 볼 수는 없기 때문에 2번 돌려보고 같으면 deterministic한 것으로 간주한다.

valid한 것으로 간주되는 경우 각 triplet은 각 task에 맞게 pair/representation 형태로 solver에게 프롬프팅된다.

Answer verification의 경우 abduction은 생성된 input을 프로그램에 입력했을 때 output이 같은지 확인하고, deduction은 생성된 output이 원래 output가 같은지 확인한다. Inductiond의 경우 prompting에 이용된 반쪽짜리 input-output과 사용되지 않은 거 모두 포함해서 program이 input에 대해 output을 잘생성하는지 확인한다.

학습의 경우 Task-Relative REINFORCE++ (TRR++) 를 사용했다. 각 sample 마다 globla baseline이 아닌 sample에 맞는 baseline을 이용해 policy를 업데이트한다. 고로, 6개의 baseline이 있을 것이다. objective는 다음과 같다.

$$\mathcal{L}_{PPO}(\theta)=\mathbb{E}_{q\sim P(Q),o\sim\pi_{old}(O\vert q)}\left[\frac{1}{\vert O\vert}\sum_{t=1}^{\vert O \vert}\min (s_t(\theta)A^{norm}_{task,role},clip(s_t(\theta),1-\epsilon,1+\epsilon)A^{norm}_{task,role})\right] $$

$$A^{norm}_{task,role}=\frac{r-\mu_{task,role}}{\sigma_{task,role}},\qquad\text{task}\in\{ind,ded,abd\},\text{role}\in\{propose,solve\}$$

# Findings
![](/assets/img/AZR/res.webp)

- AZR은 zero data 인데도 불구하고 기존의 zero reasoner들을 넘는 성능을 보이고 있다.
- Base와 coder 중 coder로 AZR 학습한게 code/math reasoning 두 분야 모두에서 더 좋은 성능을 보인다. 이는 기본적인 코딩 역량이 모델에 필요함을 말하고 있다.

![](/assets/img/AZR/res2.webp)

- 모델 사이즈가 클 수록 AZR에서 얻는 효과가 훨씬 크다. 이는 model scaling이 AZR effectiveness를 증대한다고 예측해볼 수 있다.
- Llama3.1-8B에서도 성능을 기록하고 있지만 Qwen2.5 3B에 비해 모델 사이즈가 큼에도 불구하고 크게 개선되지는 않았다.
- Reasoning pattern에서도 발견점이 있다. 예를 들어, Abduction의 경우, 여러가지 input pattern을 test해보고 주어진 input을 reasoning을 통해 output을 생성하고 입력받은 output가 매칭이 되지 않으면 다시 input을 생성하는 등 self-correction pattern을 보이고 있다. output을 생성할 때에는 program을 한 단계 한 단계 생각해보고 중간 결과를 생성하면 최종 output을 생성한다. Induction의 경우에도, 각 test case를 체크하면서 생성한 program이 test case에 맞는 결과를 생성하는지 확인한다.
- 특히 induction의 경우, intermediate planning을 reasoning과정에서 하는 것을 확인할 수 있다. Final code output에 주석 형태로 step-by-step plan이 적혀있다. 이는 math prover 모델에서 발견되는 현상과 유사하다. 이를 통해 coding 에서도 마찬가지로 long-form response를 생성하도록 유도하는게 훨씬 더 좋은 것을 알 수 있다.
- Llama에서는 특이한 cognitive pattern이 발견 되었다. 덜 똑똑한 인간들을 능가하자 식의 답변이 생성되곤 했다.
- 기존 연구들과 마찬가지로 AZR에서도 training step이 늘어날수록 token 길이가 늘어났다. 이때 task type에 따라 늘어나는 정도가 달랐다. input을 예측할 때 trial-and-error reasoning pattern을 보이고 있는데 이때 길이가 늘어나느 정도가 가장 컸다. 

# Conclusion
결론적으로, AZR은 data를 하나도 사용하지 않고 pretrained base LLM의 성능을 강화 학습을 통해 개선하는데 성공했다. 스스로 task를 생성하고 task에 대한 정답을 reasoning을 통해 스스로 생성하는 문제를 만들고 정답을 생성하는 과정을 스스로 하게 된 것이다.
