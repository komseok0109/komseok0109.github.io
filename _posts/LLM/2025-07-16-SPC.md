---
title: "[논문리뷰] SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning"
date: 2025-07-16
last_modified_at: 2025-07-16
categories:
  - 논문리뷰
tags:
  - Reinforcement Learning
  - Reasoning
  - LLM
excerpt: "SPC"
use_math: True
classes: wide
---
> arXiv 2025. [[Paper](https://arxiv.org/abs/2504.19162)] [[Code](https://github.com/chen-judge/SPC/)]
> 17 May 2025

# Introduction
Reasoning이 LLM의 성능을 크게 향상시킴으로서 reasoning의 step-level reliability 가 중요시되고 있다.(Process Verifier) 하지만 다음 3가지의 문제점으로 연구에 어려움을 겪고 있다.
- Reasoning step 의 correctness 가 애매모호하고, process verifier를 학습하기 위한 annotated step data를 확보하기 어렵다.
- human expert annotation 을 이용한다고 해도 LLM 은 빠르게 업데이트되기 때문에 최근 LLM 에 적용하기 어려울 수 있다.
- step 별 정답 여부만으로는 왜 틀렸는지 알려주는 critic 모델을 제대로 훈련시킬 수 없다.

SPC (Self-Play Critic) 는 math reasoning process 에서 sneaky generator 와 critic 사이에 self-play RL framework 를 이용해 critic을 학습시킨다. 
![](/assets/img/SPC/game.webp)
- 먼저, generator 를 SFT를 통해 학습시켜 주어진 correct reasoning step을 incorrect reasoning step으로 변환하는 모델로 학습시킨다.
- Critic 모델은 generator가 생성하는 reasoning step의 error를 찾고 critique을 제공하는 역할을 한다.
- Generator는 reasoning step의 error를 제공하고, critic은 이를 feedback하는 adversarial game을 통해 generator는 critic이 감지하기 어려운 error를 생성하도록 학습되고 critic은 reasoning step의 error를 더 잘 탐지하는 방향으로 학습된다.

# Methodology
Critic을 supervised learning으로 학습하는건 annotation의 어려움과 LLM은 빠르게 업데이트 되기 때문에 annotation 해놓은게 무용지물이 될 가능성이 높다. 또, 각 reasoning step 은 reasoning 의 결론과 다르게 정해진 답이 없다. 예를 들어 틀린 reasoning도 결론에 다다르는 과정일 수 있기 때문이다. SPC는 sneaky generator $S$ 와 step-level critic $C$ 를 이용해 이 문제를 해결한다.

![](/assets/img/SPC/overview.webp)

## Sneaky Generator
sneaky generator 는 sneaky step을 생성해 success rate 을 낮추고 critic 이 step 이 sneaky 한걸 인지하지 못하게 속이는 것을 목표로 한다: $t_k^i=S(p,\tau_{:k-1},t_k^c)$ correct step -> incorrect step. 생성된 incorrect step은 solver의 success rate을 낮추는 것에 성공한 경우 valid incorrect step $\bar{t_k^i}$ 이 된다. 만약 생성된 incorrect step 이 valid 하지 않거나 critic 이 error 를 detect 하는 경우, negative reward 를 받는다. 반대인 경우, generator 가 positivie reward 를 받는다.

Sneaky generator는 SFT 를 이용해 initialize 한다.

$$L_{SFT}=-\mathbb{E} _{(\mathbf{x,y})\sim D_{bc}^s}[\log{\pi_\theta(\mathbf{y\vert x})}]$$

$\pi_\theta$: policy (Qwen2.5-7B-Instruct)
$D_{bc}^s$: Dataset of $(\mathbf{x,y})$ where $\mathbf{x}=(p,\tau_{:k-1},t_k^c)$ as input, $\mathbf{y}=\tau_{CoT}(t_k^c)\to t_k^i$ as output. (Curated by GPT-4)
 
Critic 에 생성된 reasoning step 을 feed 하기 전에 일종의 validation 과정이 필요하다. LLM 이용해 correct step/incorrect step 이후에 reasoning을 이어가서 최종 결론까지 도출한다. 이 과정을 N번 반복하고 incorrect step이 들어갔을 때 성공률이 0% 가 되는 경우 critic에 feed한다.

## Step Critic
Critic은 reasoning trajectory $\tau_{:k-1}=(t_1,t_2,\dots,t_{k-1})$ 과 sneaky step $\bar{t_k^i}$ 이 주어졌을 때, identify 해서 reward를 결정한다.

Critic도 마찬가지로 SFT 를 이용해 initialize 한다.

$$L_{SFT}=-\mathbb{E} _{(\mathbf{x,y})\sim D_{bc}^s}[\log{\pi_\theta(\mathbf{y\vert x})}]$$

$\pi_\theta$: policy (Qwen2.5-7B-Instruct)
$D_{bc}^s$: Dataset of $(\mathbf{x,y})$ where $\mathbf{x}=(p,\tau_{:k-1},t_k)$ as input, $\mathbf{y}=\bar{Q_t}$ as output.

Dataset의 경우 $(p,\tau_{:k-1},t_k)$ 를 input으로 주고 DeepSeek-R1-Distill-Qwen-7B 모델을 이용해 critique를 생성한다. 이후에 GPT-4를 이용해 생성한 critique를 요약한 $Q_t$ 를 생성한다. 이후에 필터링을 거쳐 최종적으로 $\bar{Q_t}$ 를 dataset으로 사용한다.

## Adversarial Game
먼저, 여러 개의 모델을 이용해 step-by-step solution 을 생성한다. 이 중 random하게 solution을 골라서 generator를 이용해 sneaky transformation하고 생성된 incorrect step은 critic 에 input으로 주어지고, critic은 critique를 생성한다. Generator가 생성한 교묘한 오류를 critic이 잡아내는지 확인한다. 

$$R_{sneaky}=\begin{cases}
    1 \quad\qquad\text{Sneaky Generator Wins} \\
    -1 \qquad\text{Sneaky Generator Loses}
\end{cases} $$

$$R_{critic}=\begin{cases}
    1 \quad\qquad\text{Critic Wins} \\
    -1 \qquad\quad\text{Critic Loses}
\end{cases} $$

## Offline Reinforcement Learning

$$\nabla_\theta\hat{\mathcal{L}}(\theta)=\mathbb{E}_{\mathbf{x}\sim \mathcal{D},\mathbf{y}\sim \pi_{old}(\mathbf{y\vert x})}\left[\frac{\pi_\theta(\mathbf{y\vert x})}{\pi_{old}(\mathbf{y\vert x})}\cdot \hat{A}^{\pi_{old}}(\mathbf{x,y})\cdot\nabla_\theta \log{\pi_\theta(\mathbf{y\vert x})}\right]$$

Advantage: $\hat{A}^{\pi_{old}}(\mathbf{x,y})=R\mathbf{(x,y)}-b-\beta KL[\pi_\theta\vert\vert\pi_{ref}]$. KL penalty is added to regularize the policy and prevent it from deviating too far from the initial policy.

Generator의 경우 success rate 을 낮추고, critic이 error를 감지 못한 경우에만 positive로 간주하곤 나머지는 negative이다. Critic은 제대로 critique를 생성하지 못한 경우 negative이고 반대는 positivie이다.

각 round에서 학습된 generator/critic은 다음 단계 데이터 생성을 위해 사용된다. 이때 generator가 critic보다 약한 경우가 많아서 $C_{i-1}$와 $S_i$ 을 game하게 해서 다음 data를 생성한다.

## Enhancing LLM Reasoning
위의 방식을 이용해 학습한 critic을 이용해서 test-time search에 이용할 수 있다.


