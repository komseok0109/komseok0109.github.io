---
title: "[논문리뷰] Multi-Turn Code Generation Through Single-Step Rewards"
date: 2025-07-04
last_modified_at: 2025-07-04
categories:
  - 논문리뷰
tags:
  - Code Generation
  - LLM
excerpt: "muCode"
use_math: True
classes: wide
---
> ICML 2025. [[Paper](https://openreview.net/forum?id=aJeLhLcsh0)] [[Page](https://github.com/portal-cornell/muCode)]
> Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury

## Background
**Multi-turn code generation**: Agent가 코드를 생성하고 실행한 뒤, 실행 결과를 다음 프롬프트에 반영해서 반복적으로 코드를 수정해가며 생성하는 방식. 모든 test case를 통과하거나 maximum iteration까지 반복해서 수정한다. 이후, 성공한 코드는 private test를 이용해 correctness score를 계산한다. 이는 일종의 markov decision process로 interaction history를 state $s_t=\\{x,y_1,o_1,\dots,y_{t-1},o_{t-1}\\}$로, 생성한 코드 snippet이 action으로 생각하면 된다. Objective function:

$$\max_{\pi_{\theta}} E_{x\sim D, y_t\sim\pi_\theta (\cdot \vert s_t)}\left[\sum^T_{t=1}\gamma^tR(x,y_t)\right]$$

Reward 함수는 $C(x,y_t)\in\\{0,1\\}$ 를 이용한다 public, private test를 다 해결하면 1 아니면 0 이다. Objective function에서 알 수 있듯이 turn을 최대한 적게 사용하는게 목표이고, 임의의 step $t$ 에서 agent가 correct solution을 생성할 수 있다. 그러므로 one-step recoverable process이다. Intermediate state 어디에서든지 correct code가 생성될 수 있다.

![](/assets/img/muCode/total.webp)

## Training
![](/assets/img/muCode/train.webp)

Verifier는 프롬프트 $x$ 에 대해서 코드 solution $y$ 의 점수를 계산한다. 위 알고리즘에서 확인할 수 있듯이 $D$ 에서 optimal action (code) 를 찾는데 이용된다. Verifier는 initial prompt $x$ 와 입력으로 받은 solution 만 이용해서 계산되기 때문에 이전 state들에 dependent 하지 않고, reward label로 학습되기 된다는 장점이 있다. Breadley Terry loss를 이용해 학습한다. BCE를 사용하지 않은 이유는 verifier의 주된 목적은 각 solution들의 absolute reward보다 ranking이기 때문이다. 

Generator의 경우 verifier를 이용해 labeling 한 dataset을 이용해 fine-tuning한다. 위의 알고리즘에서 확인할 수 있듯이 verifier를 학습한 후에 verifier가 계산한 best solution을 이용해 labeling한다. 이때, 점수를 계산할 때 learned verifier 외에도 oracle verifier (groud label을 알고 있는 verifier) 를 이용해 weighted sum으로 점수를 계산한다. labeling한 dataset을 이용해 generator를 fine-tuning한다.

## Inference
![](/assets/img/muCode/infer.webp)

inference시에는 위 알고리즘에 나와 있듯이 generator를 이용해 N 개의 rollout을 생성하고 verifier를 이용해 계산한 점수가 가장 큰 solution으로 public test를 다 통과하는지 test한다. 다 통과하면 return하고 그렇지 않으면 execution feedback $o_t$ 와 best code $y_t^\*$ 를 이용해 state를 update한다. 최대 turn 수는 $T$ 이다. 이를 multi-turn Best-of-N (BON) 이라고 칭한다.

## Analysis of muCode
MDP $M=(S,A,P,R,\gamma)$ with horizon $T$ is one-step recoverable if $A^\*:=Q^\*(s,a)-V^\*(s)$ is uniformly bounded $\forall (s,a)$, i.e. $-1\leq A^\*(s,a) \leq 0$. 

Multiple code generation 에서는 optimal policy $\pi^\*(y_t,\vert s_t)$ 가 $x$ 에만 dependent하고 $s_t$ 와는 상관없다. 왜냐하면 $y_t$ 의 correctness가 $x$ 에만 dependent 하기 때문이다. 그러므로 $Q^\*(s_t,y_t)=R(x,y_t)\in\\{0,1\\}$ 이다. Optimal value function의 정의에 의해 $V^\*(s)=\max_{y_t}R(x,y_t)$ 가 된다. 그러므로 $A^\*(s_t,y_t)$ 는 uniformly bounded 되어 있고 multi code generation은 one-step recoverable하다.

- Expert: value function 을 계산할 없이 verifier 가 local search expert의 역할을 할 수 있다.
- Recoverability: 특정 state 에서 learner (generator) 가 expert (verifier) 를 imitate하는데 실패했다고 해도 다음 state에 바로 커버 가능한 구조이다. 커버가 가능하기 때문에 performance도 bound되어 있다. $\pi^\*$ 는 expert policy, $\epsilon$ 은 error, $\gamma(N)
$은 regret의 평균값이다.

$$ J(\pi^*)-J(\pi)\leq O(T(\epsilon+\gamma(N)))$$

결론적으로, muCode는 reinforcement learning 대신에 imitation learning 을 사용해 hierarchical credit assignment을 하지 않아도 된다.

