---
title: "[논문리뷰] Revisiting Chain-of-Thought in Code Generation:
 Do Language Models Need to Learn Reasoning before Coding?"
date: 2025-06-26
last_modified_at: 2025-06-26
categories:
  - 논문리뷰
tags:
  - Code Generation
  - LLM
excerpt: "CoT in code generation"
use_math: True
classes: wide
---
> ICML 2025. [[Paper](https://openreview.net/forum?id=wSZeQoJ1Vk)] [[Code]( https://github.com/richardodliu/OpenSyntheticCC)] 
> Ren-Biao Liu, Anqi Li, Chaoding Yang, Hui Sun, Ming Li
> 01 May 2025

## Introduciton
Large scale LLM을 학습하는 것은 time,cost-intensive하다. 그래서 Supervised Fine-Tuning (SFT) 을 통해 code generation등의 downstream task에 pre-trained LLM을 사용하는 방식을 이용한다. CoT (Chain of Thought)는 LLM이 final answer를 출력하기 전에 중간 reasoning step을 생성하도록 격려하는 기법이다. Fine-tuning시 CoT data를 추가해주면 모델이 사고와 설명을 통해 reliable answer를 제공한다. Code generation에서는 CoT 이용에 관한 연구가 부족한 상황이다.

- Program question의 자연어 표현, reasoning process, code solution으로 구성된 dataset 생성
- 여러 개의 LLM을 이용해 SFT 실험. Base model을 이용하기 때문에 pre-training dataset에 영향을 받지 않는다.
- Fine-tuning 동안 key paramter examination

본 연구에 따르면 코드와 CoT의 순서를 바꾸는 것이 code generation에서는 높은 성능을 기록했다. 순서를 바꿨으니 CoT는 reasoning step이 아닌 코드에 대한 설명이 된다.

## Background
**Decoder-only LLM**: $x=(x_1,x_2,\dots,x_l)$ 가 주어졌을 때, transformer-based decode layer를 거쳐 hidden state $h$를 생성한다. 마지막 hidden state를 이용하여 logits을 계산한다: $z=Linear(h_l)$. Softmax 을 이용해 다음 token을 생성하기 위한 probability distribution을 계산한다:

$$p=Softmax(z) \text{ and }p_j=\frac{\exp{z_j}}{\sum_{k=1}^{\vert V \vert}\exp{z_k}}$$

**SFT**: Labeled dataset $D=\{(x^{(i)},y^{(i)})\}_{i=1}^{\vert D \vert}$ 이 주어졌을 때 Supervised Fine-Tuning은 loss function을 minimize하는 model paramter를 찾는 과정이다.

$$\theta^*=\arg\min_{\theta}\sum_{i=1}^{\vert D\vert}\sum_{t=1}^{\vert l_i\vert}\mathcal{L}(y_{t+1}^{(i)}, Softmax(Linear(h_t^{(i)};\theta)))$$

$h_t^{(i)}$ 는 context sequence $(x^{(i)}+y_t^{(i)})$, $y_{t+1}$은 timestamp $t$ token의 encoding이다. Loss function은 대개 cross-entropy loss를 이용한다.

**Pass@k**: k번 시도했을 때 모델이 정확한 코드를 생성한 확률은 Pass@k$=1-(1-p)^k$로 수학적으로 계산된다. 실제로 계산할 수 없기 때문에 다음과 같이 estimation을 사용한다.

$$\text{Pass@k}=1-\frac{\binom{n-c}{k}}{\binom{n}{k}} $$

$k=1$이고 greedy decoding, temperature 0.0 을 사용한 경우 Pass@1은 맞춘 정답 문제의 비율과 같다.

## Method
![process](/assets/img/CoTinCG/process.webp)
Github repo에서 raw code snippet을 들고오는 경우 어렵고 다양한 code generation 문제를 생성할 가능성이 낮다. 그래서 reasoning step과 quality가 높은 코드가 부족한 dataset이 생성될 가능성이 크다. 대신에 다른 dataset을 필터링해서 수집하는 방식을 이용했다. 아래의 seed dataset을 생성한다.

$$ S=\{(x_i,y_i)\}^{\vert S\vert}_{i=1} $$

$x$는 program question, $y$는 response를 의미한다. Fine-tuning을 위해 distillation을 사용한다. Teacher model $M_t$를 이용해 data를 생성하고, student model $M_s$를 fine tuning한다. 이를 통해 labeling cost를 절약할 수 있다. 

- 먼저, $P$ 라는 예제 집합을 준비한다. 예제의 개수는 3개이다.
- Data pair $(x_i,y_i)$ 와 위의 예제를 teacher model $M_t$에 프롬프팅해 sample 몇 개를 생성한 후 quality가 가장 높은 CoT & Code를 few shot 집합 $\hat{P}$에 추가한다. 
- $\hat{P}$를 이용해 few-shot prompt를 작성해서 $M_t$ 모델로 각 seed $(x_i,y_i)$의 reasoning step $r_i$와 코드 solution $c_i$를 생성한다. 생성한 solution과 step에 대해 검증은 하지 않고 incorrect한 경우 dataset의 noise를 추가한 것으로 간주한다.
- $M_t$를 이용해 test case를 생성한다. 이후, test case를 통과한 $c_i$만 선택한다.

위 과정을 거쳐 만들어진 $D=\{(x_i,y_i,r_i,c_i)\}^{\vert D\vert}_{i=1}$ 를 이용해 base model $M_s$를 fine-tuning 한다. 4가지 서로 다른 방식을 이용해 fine-tuning한다.

- Seed Dataset ($Seed$): 기존 $S=\{(x_i,y_i)\}^{\vert S\vert}_{i=1}$ 만 이용해서 학습한 것이다 baseline으로 이용된다. 
- Code w/o CoT ($C_{w/o}$): $D=\{(x_i,c_i)\}^{\vert D\vert}_{i=1}$
- Code follow CoT ($C_{follow}$): $D=\{(x_i,r_i+c_i)\}^{\vert D\vert}_{i=1}$
- Code precede CoT ($C_{precede}$): $D=\{(x_i,c_i+r_i)\}^{\vert D\vert}_{i=1}$

## Experiments
![res](/assets/img/CoTinCG/res.webp)
![res2](/assets/img/CoTinCG/res2.webp)
![res3](/assets/img/CoTinCG/res3.webp)

차이나는 정도는 다르지만 각 benchmark에서 CoT가 코드 뒤에 나오는게 성능이 더 좋은 것을 확인할 수 있다. 코드가 reasoning process로 활용되고 CoT는 코드에 대한 설명으로 활용될 때 더 좋은 성능을 기록했다.
![base](/assets/img/CoTinCG/base.webp)

서로 다른 base model을 이용해 학습했을 때도 코드 이후 설명을 제공하는 것이 더 나은 것을 확인할 수 있다.
![res](/assets/img/CoTinCG/res4.webp)

Teacher model로 DeepSeek대신에 GPT-4o를 사용했을 때도 일관적인 결과를 보여주고 있다. 
![res](/assets/img/CoTinCG/res5.webp)

Dataset을 생성할 때 reference code를 제공하기 때문에 bias가 생길 수 있다. 그래서 코드와 관련 있는 정보 없이 teacher model 없이 지시문을 사용했다. Precede가 성능이 더 높다.
![res](/assets/img/CoTinCG/res6.webp)

뿐만 아니라, data source를 달리 했을 때도 일관적인 결과를 보여주고 있다.

## Discussions
### Model Behavior Analysis
왜 precede가 더 성능이 좋은지 분석해본다.
![ple](/assets/img/CoTinCG/per.webp)

왼쪽이 precede, 오른쪽이 follow. 두 방법론의 perplexity distribution을 비교했을 때 precede의 Code와 CoT distribution 사이의 gap이 좁다. 이는 모델이 Code생성과 CoT생성 사이의 balance를 잘 잡고 있음을 의미한다. 혼잡도의 차이가 덜하기 때문이다. CoT와 Code의 distribution이 서로 다른 이유는 Code는 syntax나 semantic이 정해져 있기 때문에 자연어로 생성하는 CoT보다 perplexity가 낮다.
![](/assets/img/CoTinCG/kl.webp)

Base모델과 SFT 모델의 KL divergence (left), validatoin loss(right)를 계산한 결과이다. KL divergence의 distribution은 크게 차이 나지 않은 것으로 보아 base모델과 SFT모델의 차이가 방법론과 관계없이 유사한 것을 알 수 있다. 하지만 validation loss의 경우 distribution의 차이가 크게 나는 것을 보았을 때 generalization 성능의 차이가 남을 확인할 수 있다.
![](/assets/img/CoTinCG/att.webp)

왼쪽이 precede, 오른쪽이 follow. Precede의 경우 CoT split에서 Code segment쪽의 attention weight이 큰 것을 확인할 수 있다. Precede일 때 CoT와 Code사이의 관계를 더 이해하려고 있음을 확인 가능하다.
![](/assets/img/CoTinCG/norm.webp)

Attention layer의 gradient의 $l_2$ norm을 분석한 결과이다. x축이 layer index를 의미한다. 왼쪽이 CoT, 오른쪽이 Code이다. Code의 gradient strength가 낮은 것으로 보아 모델이 Code파트에 대해서는 학습을 덜 민감하게 있고 학습 시 순서가 정보 제공에 큰 차이를 야기할 수 있음을 의미한다.
### Data Patern Analysis
![](/assets/img/CoTinCG/cons.webp)

Inconsistent Code는 자기가 생성한 test case를 통과하지 못한 코드를 의미한다. 왼쪽 그래프는 inconsistent data를 포함해 학습한 상황이다. self-consistent data와 비교했을 때 큰 성능 저하가 없다. 기본적으로 LLM이 생성한 test case가 항상 정확한 것도 아니고 consistency는 code quality와 직결되는 문제도 아니다. 오류를 포함하는 코드도 모델의 학습 과정에서 의미 있는 코드가 될 수 있다. 오른쪽은 signature나 inline comment를 코드에서 제거했을 때 어떤 영향을 끼치는지 확인한 그래프이다.Signature를 제거한 경우 큰 성능 저하가 발생했음을 확인할 수 있다. 모델의 학습 과정에서 함수/Class의 signature는 코드 생성에 큰 영향을 미침이 확인 가능하다. 이는 signature가 자연어로 만들어진 instruction과 프로그래밍 언어 구현 사이의 연결점이 되어주기 때문으로 분석된다.
![](/assets/img/CoTinCG/mixed.webp)

왼쪽 그래프는 mixed-data strategy를 사용했을 때를 분석한 그래프이다. single strategy를 사용했을 때보다 성능 저하가 발생했다. 이는 같은 문제에 대해 dataset이 두 개의 정답을 제공해서 모델이 학습을 어려워하는 것으로 분석된다. 오른쪽은 attacked code dataset에서 학습했을 때 결과이다. LLM을 코드를 token sequence로 보고 lexical feature에 의존하여 코드를 생성한다. 그래서 syntax와 semantic은 유지하면서 변수 이름을 아무 의미없는 이름으로 다 변경해보았을 때 성능이 떨어짐을 확인 가능하다. 우리는 코드를 작성할 때 변수명에 정보를 담아 각 변수가 어떤 역할을 하는지 나타낸다. LLM이 dataset을 보고 학습할 때도 각 변수명의 lexical feature에 접근하지 못하는 것이 성능이 저하된 것임으롭 분석 가능하다.
### Generalization Discussion
![](/assets/img/CoTinCG/large.webp)

DeepSeek-Coder-33B-Base(left), Qwen2.5-Coder-32B-Base(right). 모델 사이즈가 커져도 동일한 결과를 보여준다.
![](/assets/img/CoTinCG/res7.webp)

난이도가 달라질 때도 CoT와 Code 사이의 순서가 일관됨이 확인 가능하다. 또, CoT를 삽입하는 것이 성능을 개선하는 것도 확인할 수 있다.
![](/assets/img/CoTinCG/temp.webp)

왼쪽 그래프는 Pass@k metric을 계산할 때 k 값에 따른 성능 비교이다. k값이 커질 수록 정확도가 개선됨을 확인할 수 있다. 오른쪽 그래프는 temperature에 따른 성능 지표이다. LLM은 temperature가 낮을수록 랜덤성이 떨어진다. Temperatuer가 증가할 수록 성능이 떨어진다. 다양한 출력은 하지만 정확도는 떨어지는 것이다.

### Model Outputs Discussion
![](/assets/img/CoTinCG/strategy.webp)

LLM을 이용해 preference를 대답하게 했을 때도 precede가 CoT, Code 부문 둘 다 더 좋은 출력을 기록했다.
![](/assets/img/CoTinCG/len.webp)

더 적은 step, token으로 teacher model이 생성한 코드와 유사한 코드를 생성했음을 CodeBLEU 지표를 통해 확인 가능하다.

![](/assets/img/CoTinCG/cos.webp)
Instruction과 code(left), CoT(right)의 cosine similarity를 분석한 그래프이다.

