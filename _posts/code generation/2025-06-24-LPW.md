---
title: "[논문리뷰] Planning-Driven Programming: A Large Language Model Programming Workflow"
date: 2025-06-24
last_modified_at: 2025-06-24
categories:
  - 논문리뷰
tags:
  - Code Generation
  - LLM
excerpt: "LPW"
use_math: True
classes: wide
---
> arXiv 2024. [[Paper](https://arxiv.org/abs/2411.14503)] 
> [[Page](https://github.com/you68681/lpw)]
> Chao Lei, Yanchuan Chang, Nir Lipovetzky, Krista A. Ehinger
> 21 Nov 2024

## Introduction
LLM을 활용한 코드 생성은 단순한 자연어 생성과는 달리, 어휘적(lexical), 문법적(grammatical), 의미론적(semantic) 제약이 강해 여전히 어려운 문제로 남아 있다. 실제 개발자들은 코드를 작성한 후 테스트 케이스 실행과 디버깅을 통해 오류를 분석하고 수정하는 과정을 반복한다. 마찬가지로, LLM이 생성한 코드 역시 실행 결과 및 에러 메시지를 바탕으로 코드를 개선(refinement)하는 시도가 가능하다.

기존 연구에서는 self-debugging 기법이나 control flow graph 정보를 활용한 오류 탐지를 통해 코드 수정 가능성을 높이고자 했지만, 이러한 접근은 correction instruction이 구체적이지 않다는 한계가 있었다. 이로 인해, 모델은 의도한 해결책에서 벗어난 수정을 반복하며 비효율적인 refinement 과정을 겪게 된다. 또한, 초기 생성 코드가 주어진 문제(task)와 큰 괴리가 있는 경우, 올바른 방향으로 수정하는 것 자체가 어려운 과제로 남아 있다.

한편, 실제 소프트웨어 개발이 여러 역할의 협업으로 이루어지듯, LLM에게도 역할을 분리하여 협력시키는 multi-agent collaboration 방식이 대안으로 주목받고 있다. 예를 들어, 하나의 agent는 solution plan이나 test case를 생성하고, 다른 agent는 이에 따라 코드를 작성하거나 디버깅하는 방식으로 협업을 강화할 수 있다. 이러한 구조는 각 모델이 특정 기능에 집중함으로써 전체적인 문제 해결의 정확성과 효율성을 높일 수 있다. 하지만 solution plan/test case를 작성하는 agent의 신뢰성 문제가 있다. 또, agent의 수가 늘어날 수록 communication overhead가 증가한다.

LPW (LLM Programming Workflow)는 위 문제를 해결하기 위해 다음 두 단계로 구성된 workflow를 제시한다.
- solution generation: plan creation & plan verification
- code implementation: initial code development & subsequent refinement
  
![Pipeline](/assets/img/LPW/pipeline.webp)
LPW는 solution plan, code explanation, runtime information등을 이용해 성능을 개선하는 end-to-end framework를 제시한다. Runtime information 외의 component들은 모두 few-shot prompting을 거쳐 LLM이 생성한다.

- solution plan: solution plan이란 자연어로 제시된 task를 비교적 쉬운 sub-problem/intermediate step 으로 쪼개는 것을 이야기한다. 
- Plan verifcation: LPW는 LLM이 생성한 solution plan을 verfication하는 과정을 거친다. Verification은 llm을 이용해 intermediate output과 final output을 분석하고 test가 요구하는 결과와 일관되는지 확인한다. 
- Verification check: Verification 과정 자체도 LLM을 이용해 재검증된다. 이 과정은 실제 개발자들이 사용하는 Test-Driven Development와 유사하다.
- initial code: solution generation 단계에서 생성한 solution plan과 verification 정보를 이용해 초기 코드를 생성한다. 
- error analysis: 실패한 test case의 execution trace와 verification에서 생성한 intermediate output을 비교해 어디에 오류가 존재하는지를 분석하고 어떻게 코드를 개선해야하는지 구체적인 해결법을 제시할 수 있다.
- code explanation: LLM이 생성한 code explanation도 feedback으로 활용된다.

## Problem formulation

$$ P=<Q, T_v, T_h> $$

- $Q$: natural language problem specification concatenated with task-specific prompts
- $T=T_v\cup T_h$: set of visible and hidden test, each containing input-output pair $(t^i, t^o)$
 
LLM $M$을 이용해 모든 input-output pair에 대해 $f(t^i)=t^o$를 만족하는 $f$를 생성한다. hidden test set $T_h$는 solution generation & code implementation 단계에서는 이용되지 않는다.

## Solution Generation
![Solution](/assets/img/LPW/solution.webp)
- LLM을 이용해 Problem description $Q$ 를 solution plan $\Pi$ 로 분할한다.
- Verification 단계에서는 LLM을 이용해 visible test set $T_v$ 를 이용해 verification $A(\Pi, T_v)$ 를 생성한다. $A(\Pi, T_v)$ 는 intermediate result와 final result를 포함한 step-by-step analysis로 구성된다. 또, 각 visible test $t_v\in T_v$ 에 대해 final output $t_v^{o'}$ 과 ground-truth $t_v^o$ 를 비교하는 과정을 거친다.
- 마지막으로 verificatin check 단계에서 intermeidate result가 정확한지 다시 한 번 LLM을 이용해 확인한다.

## Code Implementation
![Code](/assets/img/LPW/code.webp)
- $Q$, $\Pi$, $A(\Pi, T_v)$ 를 프롬프팅해 초기 코드 $f$를 작성한다.
- 이후, LLM을 이용해 각 코드 line에 `print` 를 추가한다. 이를 $f_p$ 라 한다. $f_p$ 로 visible test set $T_v$를 실행한다.
- 만약 test를 전부 통과하면 hidden test set $T_h$를 이용해 $pass@1$ metric을 계산한다.
- 그렇지 않으면 첫 번째로 실패한 test case $\bar{t}_v$ 의 runtime information을 이용해 execution trace를 생성한다.
- 이후, LLM을 이용하여 Execution trace의 중간 결과들과 verification의 중간 결과들을 비교분석하고, refinement suggestion을 생성하게 한다. (Error Analysis) 
- 위에서 생성한 error analysis와 LLM을 이용해 생성한 code explanation을 하나의 prompt로 만들어 refine된 코드 $f'$를 생성하도록 프롬프팅한다.
- test -> refinement 과정을 모든 test case를 통과할 때까지 반복한다.

## Self Update
Solution generation 단계에서는 아래 두 가지 경우에서 self-correction이 일어난다.
- Verification 단계에서 $t_v^{o'}\neq t_v^o$ 일 때, solution plan의 수정이 필요하다.
- Verification check 단계에서 중간 결과 옳지 못할 때 plan verification을 재생성하라고 요청한다.
위의 self-correction 과정을 거치기 때문에 code generation을 위한 input으로 활용할 수 있다. Code implementation 단계에서는 code refinement자체가 self update라고 볼 수 있다. 이렇듯 LPW는 하나의 프로그램 $P$ 를 작성하기 위해서 solution plan, verification, 코드를 스스로 계속 수정한다. 이때, 수정 횟수가 max iteration을 넘어서면 프로그램 생성을 실패한 것으로 간주한다.

## Experiments
![Result](/assets/img/LPW/result.webp)
![Result](/assets/img/LPW/result2.webp)

**Learning from Test**
![Result](/assets/img/LPW/learn.webp)
두 벤치마크의 차이는 visible test의 개수이다. 사람이 코딩을 할 때도 다양한 예제를 보았을 때 더 나은 코드를 완성할 가능성이 높다. LLM을 활용할 때도 유사한 결과를 보임을 알 수 있다.

**Iterations**
![Result](/assets/img/LPW/iteration.webp)
- debugging이 없는 SP/baseline은 iteration수가 늘어나도 변화가 없다.
- LPW는 solution generation step을 거치기 때문에 debugging/code refinement 없이도 높은 성능을 기록하고 있다. 

**Ablation Study**
![Result](/assets/img/LPW/ablation.webp)
V (w/o verificatoin), C (w/o code implementation), S (w/o solution generation)

## Cost-Performance Analysis
![Result](/assets//img/LPW/efficiency.webp)
![Result](/assets//img/LPW/efficiency2.webp)

## Case Study
![Resu](/assets//img/LPW/case.webp)

## Limitations & Directions
- constrained by the LLM's reasoning capacity
- token overhead for plan generation and verification
- Instead of text-based solution, incorporate more advanced decompositions [[Parsel](https://arxiv.org/abs/2212.10561)]

