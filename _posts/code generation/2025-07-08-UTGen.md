---
title: "[논문리뷰] Learning to Generate Unit Tests for Automated Debugging"
date: 2025-07-08
last_modified_at: 2025-07-08
categories:
  - 논문리뷰
tags:
  - Unit Test
  - Code Generation
  - LLM
excerpt: "UTGEN & UTDEBUG"
use_math: True
classes: wide
---
> arXiv 2025 [[Paper](https://arxiv.org/abs/2505.01619)] [[Page](https://github.com/archiki/UTGenDebug)]
>  Archiki Prasad, Elias Stengel-Eskin, Zaid Khan, Justin Chih-Yao Chen, Mohit Bansal

## Introduction
Unit test 를 inference time에 생성해서 feedback으로 활용하기 위해서는 두 가지가 만족되어야 한다.
- Unit test 의 input 이 error를 trigger 하는 코드여야 한다. 즉, 너무 자명한 unit test는 아무 의미가 없다.
- Unit test 의 output 이 input 가 problem 에 맞아야 한다. 기존 연구는 reference solution 을 이용해 output 을 생성했지만 이는 자동화로 나아가는 방향이 아니고, reference solution 이 없는 일반적인 user 의 input가 맞지 않다.

이를 해결하기 위해서 unit test generator는 다음 두 가지 특성을 지녀야 한다. 이를 평가하기 위해 3가지 metric과 benchmark를 본 논문에서 제시했다.
- high attack rate: given a faulty code, UT generator should generate inputs likely to trigger errors
- high output accuracy: UT output should be consistent with the task description 

하지만 high attack rate, high output accuracy 사이에는 trade-off 가 존재한다. Attack rate 이 높을 수록 어려운 input 이니 정확한 output 을 예측하기 어렵기 때문이다. 본 논문에서는 이를 위한 dataset 을 구축하고 fine-tuning 하는 UTGEN 이라는 framework를 제시하고 있다.

LLM 으로 생성한 UT 를 debugging/feedback 에 사용하기 위해서는 LLM 이 생성했기 때문에 buggy/faulty code 를 판단하는데 실패할 가능성을 고려해야 한다. UTDEBUG 는 이러한 문제를 해결하기 위해 self-consistency 를 통해 UT 의 quality 를 높이고 UT를 여러 개 성상한 뒤 많은 UT 를 통과하는 코드를 이용한다.

## Methods
- $d$: Natural language problem description which specifies the input space $X$ and output space $Y$
- $x\in X$: valid input unit test according to $d$
- $f_r(x)=y,\forall f_r\in F_d:=\text{set of all functions that correctly solve the task}$: expected output
-  $T_\theta(d, \hat{f}_b)\to (x,y)$: automatic test generator. If $\hat{f}_b\notin F_d$, $\hat{f}_b (x)\neq y$ where $(x,y)$ is generated by $T\_\theta$, so it should generate failing tests for faulty code.  

위에서 설명한 UT generator 를 학습하는 pipeline UTGEN 을 논문에서 제시하고 있다.

![](/assets/img/UTGEN/pipeline.webp)

- 먼저, problem description $d$ 와 GT solution $f_r$ 을 수집하고 LLM 을 이용해 GT solution 을 faulty code $\hat{f}_b$ 로 만든다.
- 각 $d, f_r, \hat{f}_b$ triple 에 대해 $n$ 개의 unit test input을 sampling 한다. Unit test input $x$ 는 $f_r(x)\neq $f_b(x)$ 이면, 즉, reference output가 faulty output 이 서로 다른 경우에 failing 이라고 이야기하고 이러한 input 을 생성한다. Inference에는 GT output이 필요없지만 학습시에는 reference output 을 이용한다.
- Generator: $T_\theta(d,\hat{f_b})\to(x,CoT,y)$를 SFT하기 위한 dataset 을 구축한다. Output accuracy 와 attack rate를 둘 다 높이기 위해 UT $(x,f_r(x))$ 에 대해, LLM 을 이용해 왜 $x$ 에 해당하는 output이 $f_r(x)=y$ 인지에 대해 reasoning 을 생성한다.

즉, generator 는 faulty solution 가 problem description 이 주어졌을 때, UT input 을 생성하고 reasoning 과정을 거쳐 정확한 output $y$ 를 생성하도록 학습된다. UT 를 생성(inference)할 때에는 GT solution 을 사용하지 않아도 된다.

![](/assets/img/UTGEN/issue.webp)

학습된 generator 를 이용해 생성된 UT 는 100% 정확하지 않다. 사람이 직접 만든게 아니기 때문이다. UT 가 정확하지 않으면 faulty code $\hat{f}_b$ 이 faulty 라고 인식되지 않는 문제가 있을 수 있다. ($\hat{f}_b(x)=f_r(x)$) 혹은, output 이 틀렸을 수 있다. ($y\neq f_r(x)$) 위 두 가지 case 모두 incorrect code에 대한 디버깅을 충분히 수행하기 어렵다. 이런 문제를 해결하기 위해 UT input $x$ 에 대해 generator 는 $k$ 개의 output 을 생성하고 majority voting 을 한다. input 의 경우에도 여러 개를 생성한 이후에 majority voting 이 50% 이상을 기록한 UT만 이용한다. 

그래도 noisy feedback 은 여전히 남아있다. Noisy feedback 에 영향받지 않기 위해 debugging 후 수정된 코드가 이전에 실패한 UT 를 통과하지 못했을 때만 수정을 accept 한다. 또, $n$ 개의 UT 를 사용해 하나의 UT 만 통과하고 나머지는 통과 못하는 코드를 생성해내는 경우가 없게 한다. debugging feedback 은 하나의 UT 만 이용하는 대신에 accept의 기준을 pass rate 가 증가한 경우로 한다.

![](/assets/img/UTGEN/algo.webp)

UT 생성 알고리즘

![](/assets/img/UTGEN/algo2.webp)

디버깅 알고리즘

## Experiments
UT generation metric은 다음과 같다.

$$AttackRate=\frac{100}{\vert D \vert}\times \sum_{d\in D}\mathbb{1}_{f_r(x)\neq \hat{f}_b(x)} \text{ where } (x,y)\sim T_\theta(d,\hat{f}_b)$$

$$OutputAcc=\frac{100}{\vert D \vert}\times \sum_{d\in D}\mathbb{1}_{f_r(x)=y} \text{ where } (x,y)\sim T_\theta(d,\hat{f}_b)$$

$$Acc.\cap Attack=\frac{100}{\vert D \vert}\times \sum_{d\in D}\mathbb{1}_{f_r(x)=y, f_r(x)\neq \hat{f}_b(x)} \text{ where } (x,y)\sim T_\theta(d,\hat{f}_b)$$

![](/assets/img/UTGEN/res1.webp)

Random: no faulty code. Prompted: no fine-tuning. 위 table 에서 알 수 있듯이 faulty code 없이 program description 만 주어진 경우에는 attack rate 이 낮아 error trigger test case 를 생성해내지 못했다. Prompt 의 경우 attack rate은 높지만 output accuracy 가 낮다. fine-tuning 없이는 input 이 어려워져 output 을 제대로 생성하지 못하기 때문이다. UTGEN 의 방식이 좋음을 알 수 있다.

![](/assets/img/UTGEN/res2.webp)