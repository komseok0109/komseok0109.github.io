---
title: "[논문리뷰] Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"
date: 2025-07-07
last_modified_at: 2025-07-07
categories:
  - 논문리뷰
tags:
  - Code Generation
  - LLM
excerpt: "CURE"
use_math: True
classes: wide
---
> arXiv 2025 [[Paper](https://arxiv.org/abs/2506.03136)] [[Code](https://github.com/Gen-Verse/CURE)]
> Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang

## Introduciton
Code generation 성능을 개선하기 위해서는 unit test 생성이 key factor
- unit test 실행 결과는 correctness와 직결되므로 RL, test-time scaling, agentic coding pipeline에 이용 가능
- unit test 는 재사용 가능
- unit test 는 코드를 생성하지 않고도 problem description 만 보고도 생성 가능
  
기존에는 ground-truth code solution 를 이용해 unit test를 생성했지만 cost와 labor-intensive 문제가 존재한다. CURE는 code generator와 unit test geenrator를 co-evolve하는 RL framework로 code generator를 이용해 unit test generator를 지도하는 동시에 code generator는 정확한 unit test guidance를 위해 더 정확한 output을 생성하도록 학습된다. CURE 는 생성된 code와 test 사이의 interaction 결과를 이용해 reward matrix를 생성하고 이를 이용해 code generator와 test generator가 서로 지도하고 개선하도록 만든다. Coder가 좋은/나쁜 code를 생성하면 test generator는 둘을 구분할 수 있도록 학습된다.

![](/assets/img/CURE/evolve.webp)

## Methods
![](/assets/img/CURE/cure.webp)
### Using Unit Tests for Inference
Code generation은 unit test 의 존재로 인해 verification이 편리하다. 각 task $q$ 마다 LLM 이 $n$ 개의 candidate solution $s_j$ 을 생성하고 $m$ 개의 unit test $u_k$ 를 생성한다. candidate solution 의 unit test 실행 결과를 바탕으로 binary matrix $B\in\\{0,1\\}^{n\times m}$ 를 생성한다. 각 candidate solution 의 reward 는 $R_{s_j}=\sum_{l=1}^m B_{j,l}$ 로 정의된다. 하지만 이러한 계산 방식은 unit test 의 정확도가 낮은 경우 문제가 발생한다.

그러므로, unit test generator를 optimize 하기 위해서 아래의 objective를 사용한다.

$$\textbf{Reward Precision: } P(R_{s_{j_1}}>R_{s_{j_2}}\vert s_{j_1} \text{ is correct,} s_{j_2} \text{ is wrong}) $$

위의 overall objective 는 하나의 unit test 각각을 생성할 때는 사용할 수 없다. 논문에서는 특정 조건 아래서 unit test 를 많이 생성할 수록 reward precision 값이 1으로 수렴한다고 설명하고 있다.

![](/assets/img/CURE/theorem.webp)

- $p_u$: success probability of unit test generation
- $p_{01}$: test success probability given incorrect code and correct test
- $p_{00}$: test success probability given incorrect code and incorrect test

$\mu$가 0보다 크면 reward precision이 1로 수렴하는 것이 보장된다. $\mu$ 값이 크면 클수록 reliable reward signal을 위한 unit test 개수도 적어도 된다. 그러므로 $\mu$ 를 각 unit test generator의 objective를 사용하고 계산의 경우 execution matrix $B$ 를 이용해 estimation 한다. 

### Co-evolving Coder and Unit Tester with RL
각 task $q$ 와 ground truth unit test case $t_q$ 마다 LLM 이 $n$ 개의 candidate solution $s_j$ 을 생성하고 $m$ 개의 unit test $u_k$ 를 생성한다. Binary evaluation matrix는 $B\in\\{0,1\\}^{n\times (m+t_q)}$ 를 통해 계산된다.

각 solution $s_j$ 에 대한 reward는 ground unit test case를 많이 통과할 수록 높게 부여한다.

$$R_{s_j}^\star=\sum_{l=1}^{t_q} B_{j,m+l}^\star=I_{s_j}$$

Unit test에 대한 rewards는 $\mu$ 를 근사해서 부여한다.

$$R_{u_k}^\star=-\sum_{l=1}^n(1-I_{s_l}) B_{l,k}^\star + (\prod_{l=1}^nI_{s_l} B_{l,k}^\star )(\sum_{l=1}^n(1-I_{s_l}))$$

- $R_{u_k}^\star >0$ and propotional to the number of incorrect solutions that fail test $u_k$ when $u_k$ correctly passes all accurate solutions
- $R_{u_k}^\star < 0 $ and propotional to the number of incorrect solutions that pass $u_k$ when $u_k$ fails even one correct solution.
- Correct solution: solution that passes all ground-truth test

Unit test 를 생성할 때 correct solution을 모두 통과하는지 와 같은 단순한 reward를 사용하는 경우 pass rate를 그냥 maximize하기만 하는 너무 단순하고 자명한 test case가 만들어질 가능성이 높다. 

Rollout sample들을 생성한 후, 다음 objective를 optimize 한다.

![](/assets/img/CURE/objective.webp)


![](/assets/img/CURE/algo.webp)

### Efficiency of Long-CoT Unit Tester
Long-CoT 모델의 단점은 inference time이 너무 길다는 것이다. 이를 해결하기 위해 response-length-aware transformation을 이용한다.
