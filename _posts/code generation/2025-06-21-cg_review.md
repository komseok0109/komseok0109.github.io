---
title: "[논문리뷰] Benchmarks and Metrics for Evaluations of Code
 Generation: A Critical Review"
date: 2025-06-21
last_modified_at: 2025-06-21
categories:
  - 논문리뷰
tags:
  - Code Generation
  - LLM
excerpt: "LLM for code generation total review"
use_math: True
classes: wide
---
> arXiv 2024. [[Paper](https://arxiv.org/abs/2406.12655)] 
> Debalina Ghosh Paul, Hong Zhu, Ian Bayley 
> 18 Jun 2024

## Overview of LLMS for Coding Tasks

LLM을 이용한 코딩은 3가지 카테고리로 나뉠 수 있다.
- Description to Code (D2C)/Code Generation (CG): 자연어로 요구사항이 주어졌을 때 요구사항에 맞는 기능을 가지는 코드를 생성하는 task이다. OpenAI의 Codex를 예시로 들 수 있다. 또 다른 CG task는 자연어로 작성된 수도 코드를 주고 코드를 생성하는 pseudo-code implementation도 있다.
- Code to Description (C2D): 코드가 입력으로 주어졌을 때 자연어를 출력으로 생성하는 task이다. 코드 요악, 주석 생성 등의 task로 구성되어 있다.
- Code to Code (C2C): 입력으로 코드를 받고 출력으로도 코드를 생성하는 task이다. 
    - Code completion: 다음에 올 내용을 예측해 자동으로 코드 제안 (e.g. Copilot)
    - Code infilling: 코드 빈 부분을 자동으로 채워주는 기능 (e.g. StarCoder)
    - Code translation: 한 프로그래밍 언어로 작성된 코드를 다른 언어로 자동 변환
    - Code refactoring: 코드의 기능은 유지하면서 구조를 깔끔하게 바꾸는 작업
    - Automatic debugging/Program repair/Code repair
    - Test generation

CG/C2C 와 같이 ML 모델을 코딩에 이용하는 방법은 두 가지가 있다.
- general purpose LLM (ChatGPT, Gemini)
- special purpose LLM through fine-tuing or trainig the model from scratch

## Benchmarks
ConCode (2018)

JuICe (2019)

APPS, HumanEval, MBPP, MathQA, HumalEval+ (2021)

ClassEval, Multipl-E, DS-1000 (2023)

CoderEval, R-benchmark, Exec-CSN , EvoCodeBench(2024)
![benchmark](/assets/img/cg_review/2025-06-21-benchmark.webp)
- 데이터 소스: 코드 레포(Github), 온라인 포럼(Stack Overflow). 코드 문제 사이트(Codeforce), 외주 코딩 사이트(Upwork), 코딩 교과서, 전문가, 온라인 튜토리얼 사이트, 크라우드소싱 사이트
- Data Extraction: manual/auto/reuse existing benchmark
- Processing
  - Clarification: manual edit을 통해 자연어 표현의 모호성/불완전성 줄이기
  - Deduplication: 중복된 데이터 제거. APPS 벤치마크의 경우 tf-idf feature와 cosine similarity를 이용해 중복된 데이터를 자동으로 제거했다. tf-idf feature는 tf(각 문서에서 각 단어의 등장 빈도)와 idf(특정 단어가 등장한 문서의 수의 역수)를 곱한 값이다.
  - Decontamination: training data에 포함된 데이터 제거
- Functionality (statement/function/class/method/whole program)
![functionality](/assets/img/cg_review/2025-06-21-functionality.webp)
- Structure
![structure](/assets/img/cg_review/2025-06-21-structure.webp)
task description의 경우 자연어로 주어지고 context code나 unit test case가 데이터에 포함된 경우도 있다. 또, reference solution도 함께 제시되는 경우도 있다.

## Metrics: Functional Correctness 
Correctness의 경우 reference solution과 비교(ConCode), test case(HumanEval, MBPP, MathQA, MultiPL-E), reference solution & test case 조합 (APPS, ClassEVAL, CoderEVAL, DS-1000)을 이용해 계산된다. Reference가 제공되는 경우 test를 통과하는지 여부를 판단한다.

먼저, $T_p= \\{t_1,\cdots,t_n\\}$ 라는 프로그래밍 task $p$ 에 대한 test case 집합이 주어졌을 때, ML 모델 $M$이 생성한 코드 $P$는 $T_p$의 test case를 모두 통과한 경우 **correct** 하다고 이야기한다. 기본적으로는 벤치마크 $B$에 포함된 task중 모델이 통과한 개수 (**Accuracy**) 를 기본 평가 지표로 사용한다. 또는 각 task의 통과 비율의 평균치를 이용하기도 한다.

$$ AvgTPR_B(M)=\frac{\sum_{p\in B}TPR_{T_p}(M(p))}{\left\lVert B \right\rVert}$$

$TPR_{T_p}(M(p))$는 $T_p$ task에 대한 test pass rate를 의미한다.

## Metrics: Syntactic Closeness
Reference가 제공되는 경우 기존 자연어 연구에서와 동일하게 유사도 기반으로 모델을 평가할 수 있다.

**• BLEU**:  BLEU는 생성된 코드 $G$와 reference $R$ 사이의 n-gram(n개의 연속된 단어)를 비교한다. 매치가 되는 단어의 개수를 기반으로 precision score를 계산한다. 계산된 precision score는 생성한 코드의 길이가 짧을수록 penalty를 받는다. $T=(\tau_1,\cdots,\tau_k)$ 라는 문장이 주어졌을 때, n-gram은 $Gram_n(T)=\\{(\tau_i,\cdots,\tau_{i+n}) \mid i=1,\cdots,k-n\\}$ 으로 정의된다. 이때 precision은 다음과 같이 정의된다.

$$ p_n(G,R)=\frac{\|Gram_n(G)\cap Gram_n(R)\|}{\|Gram_n(G)\|}$$

BLEU 점수는 다음과 같이 계산된다.

$$BLEU(G,R)=BP\cdot\exp\left(\sum_{n=1}^Nw_n\log p_n(G,R)\right)$$

$w_n$은 가중치, BP는 brevity penalty이다. 생성된 코드가 짧으면 penalty를 받는다.
$$ BP=\begin{cases}
  1 \text{ if } c>r \\
  e^{(1-\frac{r}{c})} \text{ if } c\leq r
\end{cases}$$
$c$는 생성한 코드의 길이이고, $r$은 reference의 길이이다. BLEU 값이 클수록 생성된 코드가 reference와 일치하는 정도가 크다.

**• ROUGE** : ROUGE-N은 생성한 코드 $G$와 reference의 집합 $R$간의 유사도를 계산한다.
 
$$ ROUGE_N(G,R)=\frac{\sum_{S\in R}\|Gram_n(S)\cap Gram_n(G)\|}{\sum_{S\in R}\|Gram_n(S)\|}$$

ROUGE-L은 $G$와 $R$간의 longest common subsequence(LCS)를 측정하는 지표이다. Precision은 생성한 코드 중 얼만큼이 reference와 일치하는지를, recall은 생성한 코드가 실제 reference에 비해 얼만큼 일치하는지를 나타내는 지표이다. Precision과 recall을 이용해 ROUGE-L이 계산된다.

$$\text{Precision: }P_{lcs}(G,R)=\frac{LCS(G,R)}{len(G)}, \text{ Recall: }R_{lcs}(G,R)=\frac{LCS(G,R)}{len(R)} $$

$$ROUGE_L(G,R)=\frac{(1+\beta^2)\cdot P_{lcs}(G,R)\cdot R_{lcs}(G,R)}{R_{lcs}(G,R)+\beta^2\cdot P_{lcs}(G,R)}$$

ROUGE-W/ROUGE-S/ROUGE-SU는 code generation분야에서는 잘 사용되지 않는다고 한다.

**• METEOR** : METEOR metric에서 생성된 코드와 reference는 다양한 기준을 사용하여 비교된다.
  - Exact word match: 생성된 코드와 reference 사이 정확히 match되는 개수 
  - Stemmed match: 어근이 같은 단어의 개수
  - Synonym match: WordNet기반으로 한 유의어 매칭
  - Phrase match: table기반으로 한 구문 수준 매칭
위의 match 방식을 이용해 $G$와 $R$사이의 match되는 단어의 집합을 $MatchWords(G,R)$이라고 할 때, precision, recall은 다음과 같이 정의된다.

$$ Prec(G,R) = \frac{\|MatchWords(G,R)\|}{\|G\|}$$

$$ Rec(G,R) = \frac{\|MatchWords(G,R)\|}{\|R\|}$$

METEOR는 precision과 recall의 harmonic mean을 이용하여 계산된다.

$$METEOR = F_mean\cdot(1-Penalty)$$

Penalty는 $G$와 $R$의 단어 순서가 차이가 많이 날 수록 큰 값을 가진다.

**• CHRF**  (Chracter n-gram F-score): ChrF는 단어 대신 문자 기준 n-gram을 비교한다. 계산 방식은 ROUGE-L와 유사하다.

$$ChrF=(1+\beta^2)\cdot\frac{Prec\cdot Rec}{\beta^2 \cdot Prec + Rec} $$

$Prec$, $Rec$는 precision, recall을 의미하고 $\beta$는 recall과 precision사이의 균형을 유지하는 parameter이다.

지금까지는 기존 자연어 처리에서 사용되는 metric이었고 Ruby와 CodeBLUE는 code generation에 사용하기 위해 만들어진 metric이다.

**• RUBY** : 
$$RUBY(G, R)=\begin{cases}
  GRS(G,R) \text{ if PDGs are applicable,} \\
  TRS(G,R) \text{ if ASTs are applicable,} \\
  STS(G,R) \text{ otherwise} 
\end{cases}$$
GRS, TRS는 각각 program dependency graph간의 유사도, abstract syntax tree간의 유사도를 의미한다. STS는 두 코드 간의 string edit distance를 의미한다. String edit distance는 하나의 string을 다른 string으로 변환하기 위해 edit이 몇 번 필요한지를 계산한 값이다.

**• CodeBLEU**: CodeBLEU는 BLEU, Weighted-N-gram, AST-Match, DataFlowMatch의 weighted sum으로 계산된다.

$$CodeBLEU=\alpha\cdot BLEU+\beta\cdot WeightedNgram+\gamma\cdot ASTMatch +\delta\cdot DataFlowMatch $$

Weighted n-gram match는 기존 n-gram matching에서 keyword, identifer, operator등 코딩에 알맞게 가중치를 추가한 것이다. AST-match는 Abstract Syntax Tree간의 유사도를 계산한 것이다. DataFlowMatch는 두 코드간의 data flow graph의 유사도를 계산한 것이다.

**Validity**: Correctness metric의 경우 Code generation 분야에 알맞는 직관적인 metric이지만 유사도 기반의 metric은 그렇지 않을 수 있다. 실제로 BLEU의 경우 functional correctness와의 correlation이 음수 값을 가짐이 확인되었다. 6개의 유사도 기반 metric 모두 값이 5 미만 차이 날 때는 CG측면에서 큰 개선이 이루어지지 않은 것으로 판단되었고 CG 전용으로 만들어진 CodeBLEU, RUBY는 기존 metric보다 더 좋지 못했다. 그래서 현재 code generation 분야는 대개 accuracy를 metric으로 사용한다.

## Metrics: Usability and Productivity
Usability는 code generation tool 혹은 모델이 사용자의 요구를 얼마나 충족시키는지에 대한 지표이다. "Correct" 여부보다는 생성된 코드가 사용자가 이용하기 적합한지를 나타낸다. 사용자가 이용하기 적합하기 위해서는 이해가능해야 하고 논리적이고 구조가 명확해야 한다. 수정이 쉬워야 하며 간결하고 완성된 코드여야 하며 정확해야 한다. 코드와 함께 생성된 텍스트 설명도 충분히 설명이 되어 있고 읽기 쉬워야 한다. 1~5의 척도로 reference와 비교해서 평가한다. 사용자가 답을 얻기까지 걸린 시도 횟수나 완료 시간도 평가에 포함될 수 있다. 마지막으로, 학습하기 쉬운 정도도 usability에 포함된다.

Productivity는 task completion time, product quality, cognitive load, enjoyment, learning등을 포함한다. 예를 들어 Copilot 사용자의 행동 데이터를 수집하고 사용자 설문 조사 결과와 비교해보았다. Copilot이 제안한 코드의 수용률이 생산성을 예측하는데 가장 좋은 지표이다. 또 python IDE인 PyCharm은 생성한 코드의 작업 완료 시간과 정확도 점수를 인터넷에서 검색한 코드와 비교하여 생산성을 평가했다.

## Metrics: Multi-Trial vs Multi-Attempt Metrics
$pass@k$ metric은 특정 ML 모델이 주어진 코딩 문제에 대해서 $k>0$번 solution을 생성했을 때 하나의 solution이라도 성공할 확률을 나타낸다. 즉, $k$번의 시도 중 최소 1개가 성공할 확률을 나타내는 지표이다. 단순 계산은 variation이 커서 최소 1개 성공했는지 여부를 기록하는 것이 아니라, $k$번 시도 중 성공한 시도의 개수 $c$를 기록해서 $k$와 $c$값을 이용해 통계적 추정 방법으로 $pass@k$를 계산한다. $pass@k$는 한 모델을 같은 문제에 여러 번 사용할 때를 평가하는 지표이기에 multi-trial metric이라고 불린다.
반면, ChatGPT와 같은 LLM모델은 사용자와 상호작용하며 정답에 도달하기 때문에 단순 multi-trial이 아닌 만족스러운 정답을 얻을 때까지 여러번 시도를 해보는 쪽으로 해석이 가능하다. 그런 관점에서 metric으로서 $\\# attempt_k$는 사용자가 정답을 얻기 위해 시도한 횟수의 평균으로 사용자가 정답을 얻거나, $k$ 번 시도하고 실패하면 멈춘다.

![metrics](/assets/img/cg_review/2025-06-21-metrics.webp)

각 벤치마크에서는 주로 pass@k를 metric으로 사용하고 있는 것을 확인할 수 있다.

## Research Directions
- 위 table에서 볼 수 있듯이 각 벤치마크는 대개 하나의 소스를 사용해 데이터를 추출했다. 그러므로 코드 생성 질의에 대한 다양성이 부족하고 데이터 분포가 치우쳐 있다.
- 위에서 확인했듯이 평가를 할 때 각 task의 test들을 통과했는지 여부를 metric으로 사용한다. 이는 매우 객관적이고 자동으로 평가가 가능한 지표지만 test의 적합성에 크게 의존하는 경향이 있다. 그러기 위해서 특정 벤치마크는 branch coverage를 넓게 제공하거나 가능한 corner case 모두를 test에 포함시켜서 평가의 신뢰도를 높이려고 시도하고 있다. test case를 사용하는 방식의 또 다른 문제점은 코드가 구현하지 못한 기능은 검출가능하지만 악성 코드는 탐지가 어렵다는 것이다. test를 통과해도 부정확하거나 위험한 코드일 수 있다.
- $pass@k$는 LLM 출력의 랜덤성을 반영하는 지표이지만 실제 사용자는 LLM을 여러 번 돌려보지 않을 가능성이 존재한다. 그런 관점에서 $\\#attempt_k$가 나아볼 수 있지만 만족할 때까지 돌려본다는 것 자체가 manual하고 주관이 담길 수 밖에 없기 때문에 규모가 큰 실험에서는 사용하기 어렵다.
- 유사도 기반 metric도 개선되어야 한다. 유사도 기반 metric으로 correctness를 측정하기 어렵기 때문에 usability를 측정하는 방향으로 개선되어야 한다.
- 현재 제시되어 있는 벤치마크들은 시나리오 기반 평가가 부족하다. pass@k와 같은 단일 평가 방식은 어떤 시나리오에서 잘하고 어떤 시나리오에서는 성능이 좋지 않은지 구분하기 어렵다. 시나리오별 성능 평가를 하면 피드백하기도 쉽다. metadata를 포함하면 되지만 각 task마다 일일히 metadata를 부여하는 것은 많은 노력이 필요하기에 자동으로 하는 것이 도전과제이다.

## Conclusion
Code generation은 벤치마크, 평가지표에 대한 타당성에서 많은 개선이 필요하다. 평가를 자동화하기 위한 기술 개발도 필요한 상황이다.

## References
[1] Paul, Debalina Ghosh, Hong Zhu, and Ian Bayley. "Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review." 2024 IEEE International Conference on Artificial Intelligence Testing (AITest). IEEE, 2024.