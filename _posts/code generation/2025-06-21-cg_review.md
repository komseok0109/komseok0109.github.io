---
title: "[논문리뷰] Benchmarks and Metrics for Evaluations of Code
 Generation: A Critical Review"
date: 2025-06-21
last_modified_at: 2025-06-21
categories:
  - 논문리뷰
tags:
  - Code Generation
  - LLM
excerpt: "LLM for code generation total review"
use_math: True
classes: wide
---
> arXiv 2024. [[Paper](https://arxiv.org/abs/2406.12655)] 
> Debalina Ghosh Paul, Hong Zhu, Ian Bayley 
> 18 Jun 2024

## Overview of LLMS for Coding Tasks

LLM을 이용한 코딩은 3가지 카테고리로 나뉠 수 있다.
- Description to Code (D2C)/Code Generation (CG): 자연어로 요구사항이 주어졌을 때 요구사항에 맞는 기능을 가지는 코드를 생성하는 task이다. OpenAI의 Codex를 예시로 들 수 있다. 또 다른 CG task는 자연어로 작성된 수도 코드를 주고 코드를 생성하는 pseudo-code implementation도 있다.
- Code to Description (C2D): 코드가 입력으로 주어졌을 때 자연어를 출력으로 생성하는 task이다. 코드 요악, 주석 생성 등의 task로 구성되어 있다.
- Code to Code (C2C): 입력으로 코드를 받고 출력으로도 코드를 생성하는 task이다. 
    - Code completion: 다음에 올 내용을 예측해 자동으로 코드 제안 (e.g. Copilot)
    - Code infilling: 코드 빈 부분을 자동으로 채워주는 기능 (e.g. StarCoder)
    - Code translation: 한 프로그래밍 언어로 작성된 코드를 다른 언어로 자동 변환
    - Code refactoring: 코드의 기능은 유지하면서 구조를 깔끔하게 바꾸는 작업
    - Automatic debugging/Program repair/Code repair
    - Test generation

CG/C2C 와 같이 ML 모델을 코딩에 이용하는 방법은 두 가지가 있다.
- general purpose LLM (ChatGPT, Gemini)
- special purpose LLM through fine-tuing or trainig the model from scratch

## Benchmarks
![benchmark](/assets/img/2025-06-21-benchmark.webp)
- 데이터 소스: 코드 레포(Github), 온라인 포럼(Stack Overflow). 코드 문제 사이트(Codeforce), 외주 코딩 사이트(Upwork), 코딩 교과서, 전문가, 온라인 튜토리얼 사이트, 크라우드소싱 사이트
- Data Extraction: manual/auto/reuse existing benchmark
- Processing
  - Clarification: manual edit을 통해 자연어 표현의 모호성/불완전성 줄이기
  - Deduplication: 중복된 데이터 제거. APPS 벤치마크의 경우 tf-idf feature와 cosine similarity를 이용해 중복된 데이터를 자동으로 제거했다. tf-idf feature는 tf(각 문서에서 각 단어의 등장 빈도)와 idf(특정 단어가 등장한 문서의 수의 역수)를 곱한 값이다.
  - Decontamination: training data에 포함된 데이터 제거
- Functionality (statement/function/class/method/whole program)
![functionality](/assets/img/2025-06-21-functionality.webp)
- Structure
![structure](/assets/img/2025-06-21-structure.webp)
task description의 경우 자연어로 주어지고 context code나 unit test case가 데이터에 포함된 경우도 있다. 또, reference solution도 함께 제시되는 경우도 있다.

## Metrics: Functional Correctness 
Correctness의 경우 reference solution과 비교(ConCode), test case(HumanEval, MBPP, MathQA, MultiPL-E), reference solution & test case 조합 (APPS, ClassEVAL, CoderEVAL, DS-1000)을 이용해 계산된다. Reference가 제공되는 경우 test를 통과하는지 여부를 판단한다.

먼저, $T_p= \\{t_1,\cdots,t_n\\}$ 라는 프로그래밍 task $p$ 에 대한 test case 집합이 주어졌을 때, ML 모델 $M$이 생성한 코드 $P$는 $T_p$의 test case를 모두 통과한 경우 **correct** 하다고 이야기한다. 기본적으로는 벤치마크 $B$에 포함된 task중 모델이 통과한 개수 (**Accuracy**) 를 기본 평가 지표로 사용한다. 또는 각 task의 통과 비율의 평균치를 이용하기도 한다.

$$ AvgTPR_B(M)=\frac{\sum_{p\in B}TPR_{T_p}(M(p))}{\left\lVert B \right\rVert}$$

$TPR_{T_p}(M(p))$는 $T_p$ task에 대한 test pass rate를 의미한다.

## Metrics: Syntactic Closeness
Reference가 제공되는 경우 기존 자연어 연구에서와 동일하게 유사도 기반으로 모델을 평가할 수 있다.

**• BLEU**:  BLEU는 생성된 코드 $G$와 reference $R$ 사이의 n-gram(n개의 연속된 단어)를 비교한다. 매치가 되는 단어의 개수를 기반으로 precision score를 계산한다. 계산된 precision score는 생성한 코드의 길이가 짧을수록 penalty를 받는다. $T=(\tau_1,\cdots,\tau_k)$ 라는 문장이 주어졌을 때, n-gram은 $Gram_n(T)=\\{(\tau_i,\cdots,\tau_{i+n}) \mid i=1,\cdots,k-n\\}$ 으로 정의된다. 이때 precision은 다음과 같이 정의된다.

$$ p_n(G,R)=\frac{\|Gram_n(G)\cap Gram_n(R)\|}{\|Gram_n(G)\|}$$

BLEU 점수는 다음과 같이 계산된다.

$$BLEU(G,R)=BP\cdot\exp\left(\sum_{n=1}^Nw_n\log p_n(G,R)\right)$$

$w_n$은 가중치, BP는 brevity penalty이다. 생성된 코드가 짧으면 penalty를 받는다.
$$ BP=\begin{cases}
  1 \text{ if } c>r \\
  e^{(1-\frac{r}{c})} \text{ if } c\leq r
\end{cases}$$
$c$는 생성한 코드의 길이이고, $r$은 reference의 길이이다. BLEU 값이 클수록 생성된 코드가 reference와 일치하는 정도가 크다.

**• ROUGE** : ROUGE-N은 생성한 코드 $G$와 reference의 집합 $R$간의 유사도를 계산한다.
 
$$ ROUGE_N(G,R)=\frac{\sum_{S\in R}\|Gram_n(S)\cap Gram_n(G)\|}{\sum_{S\in R}\|Gram_n(S)\|}$$

ROUGE-L은 $G$와 $R$간의 longest common subsequence(LCS)를 측정하는 지표이다. Precision은 생성한 코드 중 얼만큼이 reference와 일치하는지를, recall은 생성한 코드가 실제 reference에 비해 얼만큼 일치하는지를 나타내는 지표이다. Precision과 recall을 이용해 ROUGE-L이 계산된다.

$$\text{Precision: }P_{lcs}(G,R)=\frac{LCS(G,R)}{len(G)}, \text{ Recall: }R_{lcs}(G,R)=\frac{LCS(G,R)}{len(R)} $$

$$ROUGE_L(G,R)=\frac{(1+\beta^2)\cdot P_{lcs}(G,R)\cdot R_{lcs}(G,R)}{R_{lcs}(G,R)+\beta^2\cdot P_{lcs}(G,R)}$$

ROUGE-W/ROUGE-S/ROUGE-SU는 code generation분야에서는 잘 사용되지 않는다고 한다.

**• METEOR** : METEOR metric에서 생성된 코드와 reference는 다양한 기준을 사용하여 비교된다.
  - Exact word match: 생성된 코드와 reference 사이 정확히 match되는 개수 
  - Stemmed match: 어근이 같은 단어의 개수
  - Synonym match: WordNet기반으로 한 유의어 매칭
  - Phrase match: table기반으로 한 구문 수준 매칭
위의 match 방식을 이용해 $G$와 $R$사이의 match되는 단어의 집합을 $MatchWords(G,R)$이라고 할 때, precision, recall은 다음과 같이 정의된다.

$$ Prec(G,R) = \frac{\|MatchWords(G,R)\|}{\|G\|}$$

$$ Rec(G,R) = \frac{\|MatchWords(G,R)\|}{\|R\|}$$

METEOR는 precision과 recall의 harmonic mean을 이용하여 계산된다.

$$METEOR = F_mean\cdot(1-Penalty)$$

Penalty는 $G$와 $R$의 단어 순서가 차이가 많이 날 수록 큰 값을 가진다.

**• CHRF**  (Chracter n-gram F-score): ChrF는 단어 대신 문자 기준 n-gram을 비교한다. 계산 방식은 ROUGE-L와 유사하다.

$$ChrF=(1+\beta^2)\cdot\frac{Prec\cdot Rec}{\beta^2 \cdot Prec + Rec} $$

$Prec$, $Rec$는 precision, recall을 의미하고 $\beta$는 recall과 precision사이의 균형을 유지하는 parameter이다.

지금까지는 기존 자연어 처리에서 사용되는 metric이었고 Ruby와 CodeBLUE는 code generation에 사용하기 위해 만들어진 metric이다.

**• RUBY** : 
$$RUBY(G, R)=\begin{cases}
  GRS(G,R) \text{ if PDGs are applicable,} \\
  TRS(G,R) \text{ if ASTs are applicable,} \\
  STS(G,R) \text{ otherwise} 
\end{cases}$$
GRS, TRS는 각각 program dependency graph간의 유사도, abstract syntax tree간의 유사도를 의미한다. STS는 두 코드 간의 string edit distance를 의미한다. String edit distance는 하나의 string을 다른 string으로 변환하기 위해 edit이 몇 번 필요한지를 계산한 값이다.

**• CodeBLEU**: CodeBLEU는 BLEU, Weighted-N-gram, AST-Match, DataFlowMatch의 weighted sum으로 계산된다.

$$CodeBLEU=\alpha\cdot BLEU+\beta\cdot WeightedNgram+\gamma\cdot ASTMatch +\delta\cdot DataFlowMatch $$

Weighted n-gram match는 기존 n-gram matching에서 keyword, identifer, operator등 코딩에 알맞게 가중치를 추가한 것이다. AST-match는 Abstract Syntax Tree간의 유사도를 계산한 것이다. DataFlowMatch는 두 코드간의 data flow graph의 유사도를 계산한 것이다.

**Validity**: Correctness metric의 경우 Code generation 분야에 알맞는 직관적인 평가지표지만 유사도 기반의 평가지표는 그렇지 않을 수 있다. 실제로 BLEU의 경우 functional correctness와의 correlation이 음수 값을 가짐이 확인되었다.
## References
[1] Paul, Debalina Ghosh, Hong Zhu, and Ian Bayley. "Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review." 2024 IEEE International Conference on Artificial Intelligence Testing (AITest). IEEE, 2024.